# Site Reliability Engineering

## Introduction

### The Sysadmin Approach to Service Management

Historically, companies have employed systems administrators to run complex computing systems.

The sysadmin model of service management has several advantages. For companies deciding how to run and staff a service, this approach is relatively easy to implement: as a familiar industry paradigm, there are many examples from which to learn and emulate. An array of existing tools, software components, and integration companies are available to help run those assembled systems, so a novice sysadmin team doesn't have to reinvent the wheel and design a system from scratch.

The sysadmin approach and the accompanying development/ops split has a number of disadvantages and pitfalls. These fall broadly into two categories: direct costs and indirect costs.

- Direct costs are neither subtle nor ambiguous. Running a service with a team that relies on manual intervention for both change management and event handling becomes expensive as the service and/or traffic to the service grows, because the size of the team necessarily scales with the load generated by the system.

- The indirect costs of the development/ops split can be subtle, but are often more expensive to the organization than the direct costs. These costs arise from the fact that the two teams are quite different in background, skill set, and incentives. They use different vocabulary to describe situations; they carry different assumptions about both risk and possibilities for technical solutions; they have different assumptions about the target level of product stability.

Traditional operations teams and their counterparts in product development thus often end up in conflict, most visibly over how quickly software can be released to production. At their core, the development teams want to launch new features and see them adopted by users. At their core, the ops teams want to make sure the service doesn't break while they are holding the pager. Because most outages are caused by some king of change - a new configuration, a new feature launch, or a new type of user traffic - the two teams' goals are fundamentally in tension.

The ops team attempts to safeguard the running system against the risk of change by introducing launch and change gates.

### Google's Approach to Service Management: Site Reliability Engineering

Conflict isn't an inevitable part of offering a software service. Google has chosen to run their systems with a different approach: Their Site Reliability Engineering teams focus on hiring software engineers to run their products and to create systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.

SRE is what happens when you ask a software engineer to design an operations team. The result of Google's approach to hiring for SRE is that whey end up with a team of people who will quickly become bored by performing tasks by hand, and have the skill set necessary to write software to replace their previously manual work, even when the solution is complicated.

By design, it is crucial that SRE teams are focused on engineering. Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload. Google places a 50% cap on the aggregate "ops" work for all SREs - tickets, on-call, manual tasks, etc. This cap ensures that the SRE team has enough time in their schedule to make the service stable and operable. This cap is an upper bound; over time, left to their own devices, the SRE team should end up with very little operational load and almost entirely engage in development tasks, because the service basically runs and repairs itself: we want systems that are automatic, not just automated. In practice, scale and new features keep SREs on their toes.

Google's rule of thumb is that an SRE team must spend the remaining 50% of its time actually doing development.

Google SRE's approach to running large-scale systems has many advantages. Because SREs are directly modifying code in their pursuit of making Google's systems run themselves, SRE teams are characterized by both rapid innovation and a large acceptance of change. The number of SREs needed to run, maintain, and improve a system scales sublinearly with the size of the system. Finally, not only does SRE circumbent the dysfunctionality of the dev/ops split, but this structure also improves our product development teams: easy transfers between product development and SRE teams cross-train the entire group, and improve skills of developers who otherwise may have difficulty learning how to build a million-core distributed system.

> Devops or SRE? One could view DevOps as a generalization of several core SRE principles to a wider range of organizations, management structures, and personnel. One could equivalently view SRE as a specific implementation of DevOps with some idiosyncratic extensions.

### Tenets of SRE

In general, an SRE team is responsible for the availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning of their services.

#### Ensuring a Durable Focus on Engineering

In practice, this is accomplished by monitoring the amount of operational work being done by SREs, and redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, (re)integrating developers into on-call pager rotations, and so on.

When they are focused on operations work, on average, SREs should receive a maximum of two events per 8-12-hour on-call shift. This target volume gives the on-call engineer enough time to handle the event accurately and quickly, clean up and restore normal service, and then conduct a postmortem.

Postmortems should be written for all significant incidents, regardless of whether or not they pages; portmortems that did not trigger a page are even more valuable, as they likely point to clear monitoring gaps. This investigation should establish what happened in detail, find all root causes of the event, and assign actions to correct the problem or improve how it is addressed next time. Google operates under a blame-free postmortem culture, with a goal of exposing faults and applying engineering to fix these faults, rather than avoiding of minimizing them.

#### Pursuing Maximum Change Velocity Without Violating a Service's SLO

The error budget stems from the observation that 100% is the wrong reliability target for basically everything (pacemakers and ati-lock brakes being notable exceptions). In general, for any software service or system, 100% is not the right reliability target because no user can tell the difference between a system being 100% available and 99.999% available. There are many other systems in the path between user and service (their laptop, their home WiFi, their ISP, the power grid...) and those systems collectively are far less than 99.999% available. Thus, the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.

What, then, is the right reliability target for the system? This actually isn't a technical question at all - it's a product question, which should take the following considerations into account:

- What level of availability will the users be happy with, given how they use the product?

- What alternatives are available to users who are dissatisfied with the product's availability?

- What happens to user's usage of the product at different availability levels?

The business or the product must establish the system's availability target. Once that target is established, the error budget is one minus the availability target. A service that's 99.99% available is 0.01% unavailable. That permitted 0.01% unavailability is the service's error budget. We can spend the budget on anything we want, as long as we don't overspend it.

So how to we want to spend the error budget? The development team wants to launch features and attract new users. Ideally, we would spend all of our error budget taking risks with things we launch in order to launch them quickly. This basic premise describes the whole model of error budgets. As soon as SRE activities are conceptualized in this framework, freeing up the error budget through tactics such as phased wollouts and 1% experiments can optimize for quicker launches.

The use of an error budget resolves the structural conflict of incentives between development and SRE. SRE's goal is no longer "zero outages"; rather, SREs and product developers aim to spend the error budget getting maximum feature velocity. This change makes all the difference. An outage is no longer a "bad" thing - it is an expected part of the process of innovation, and an occurrence that both development and SRE teams manage rather than fear.

#### Monitoring

Monitoring is one of the primary means by which service owners keep track of a system's health and availability. As such, monitoring strategy should be constructed thoughtfully. A classic and common approach to monitoring is to watch for a specific value or condition, and then to trigger an email alert when that value is exceeded or that condition occurs. However, this type of email alerting is not an effective solution: a system that requires a human to read an email and decide whether or not some type of action needs to be taken in response is fundamentally flawed. Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.


There are three kinds of valid monitoring output:

- **Alerts:** Signify that a human needs to take action immediately in response to something that is either happening or about to happen, in order to improve the situation.

- **Tickets:** Signify that a human needs to take action, but not immediately. The system cannot automatically handle the situation, but if a human takes action in a few days, no damage will result.

- **Logging:** No one needs to look at this information, but it is recorded for diagnostic or forensic purposes. The expectation is that no one reads logs unless something else prompts them to do so.

#### Emergency Response

Humans add latency. Even if a given system experiences more actual failures, a system that can avoid emergences that require human intervention will have higher  availability than a system that requires hands-on intervention. When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a "playbook" produces roughly a 3x improvement in MTTR as compared to the strategy of "winging it".

The hero jack-of-all-trades on-call engineer does work, but the practiced on-call engineer armed with a playbook works much better.

While no playbook, no matter how comprehensive it may be, is a substitute for smart engineers able to think on the fly, clear and through troubleshooting steps and tips are valuable when responding to a high-stakes or time-sensitive page. Thus, Google SRE relies on on-call playbooks, in addition to exercise such as the "Wheel of Misfortune", to prepare engineers to react to on-call events.

#### Change Management

SRE has found that roughly 70% of outages are due to changes in a live system. Best practices in this domain use automation to accomplish the following:

- Implementing progressive rollouts.

- Quickly and accurately detecting problems.

- Rolling back changes safely when problems arise.

By removing humans from the loop, these practices avoid the normal problems of fatigue, familiarity/contempt, and inattention to highly repetitive tasks. As a result, both release velocity and safety increase.

#### Demand Forecasting and Capacity Planning

It can be viewed as ensuring that there is sufficient capacity and redundancy to serve projected future demand with the required availability. There's nothing particularly special about these concepts, except that a surprising number of services and teams don't take the steps necessary to ensure that the required capacity is in place by the time it is needed.

Capacity planning should take both organic growth  (which stems from natural product adoption and usage by customers) and inorganic growth (which results from events like feature launches, marketing campaigns, or other business-driven changes) into account.

Several steps are mandatory in capacity planning:

- An accurate organic demand forecast which extends beyond the lead time required for acquiring capacity.

- An accurate incorporation of inorganic demand sources into the demand forecast.

- Regular load testing of the system to correlate raw capacity (servers, disks, and so on) to service capacity.

Because capacity is critical to availability, it naturally follows that SRE team must be in charge of capacity planning, which means they also must be in charge of provisioning.

#### Provisioning

Provisioning combines both change management and capacity planning. In our experience, provisioning must be conducted quickly and only when necessary, as capacity is expensive.

This exercise must also be done correctly or capacity often involves spinning up a new instance or location, making significant modification to existing systems (configuration files, load balancers, networking), and validating that the new capacity performs and delivers correct results.

#### Efficiency and Performance

Efficient use of resource is important any time a service cares about money. Because SRE ultimately controls provisioning, it must also be involved in any work on utilization, as utilization is a function of how a given service works and how it is provisioned. It follows that paying close attentions to the provisioning strategy for a service, and therefore its utilization, provides a very, very big lever on the service's total costs.

Resource use is a function of demand (load), capacity, and software efficiency. SREs predict demand, provision capacity, and can modify the software. These three factors are a large part (though not the entirety) or a service's efficiency.

Software systems become slower as load is added to them. A slowdown in a service equates to a loss of capacity. At some point, a slowing system stops serving, which corresponds to infinite slowness. SREs provision to meet a capacity target at a specific response speed, and thus are keenly interested in a service's performance. SREs and product developers will (and should) monitor and modify a service to improve its performance, thus adding capacity and improving efficiency.

## Embracing Risk

### Managing Risk

Unreliable systems can quickly errode users' confidence, so we want to reduce the change of system failure. However, experience shows that as we build systems, cost does not increase linearly as reliability increments - an incremental improvement in reliability may cost 100x more than the previous increment.

The costliness has two dimensions:

- **The cost of redundant machine/compute resources:** The cost associated with redundant equipment that, for example, allows us to take systems offline for routine or unforeseen maintenance, or provides space for us to store parity code blocks that provide a minimum data durability guarantee.

- **The opportunity cost:** The cost borne by an organization when it allocates engineering resources to build systems or features that diminish risk instead of features that are directly visible to or usable by end users. These engineers no longer work on new features and products for end users.

In SRE, we manage service reliability largely by managing risk. We conceptualize risk as continuum.  In a sense, we view the availability target as both a minimum and a maximum. The key advantage of this framing is that it unlocks explicit, thoughtful risk taking.

### Measuring Service Risk

By setting a target, we can assess our current performance and track improvements or degradations over time. For service risk, it is not immediately clear ho to reduce all of the potential factors into a single metric. Service failures can have many potential effects, including user dissatisfaction, harm, or loss of trust; direct or indirect revenue loss; brand or reputational impact; and undesirable press coverage.

Clearly, some of these factors are very hard to measure. To make this problem tractable and consistent across many types of systems we run, we focus on unplanned downtime.

For most services, the most straightforward way of representing risk tolerance is in terms of the acceptable level of unplanned downtime. Unplanned downtime is captured by the desired level of service availability, usually expressed in terms of the number of "nines" we would like to provide: 99.9%, 99.99%, or 99.999% availability. Each additional nine corresponds to an order of magnitude improvement toward 100% availability.

For serving systems, this metric is traditionally calculated based on the proportion of system uptime:

```
                     uptime
availability = -------------------
               (uptime + downtime)
```

Using this formula over the period of a year, we can calculate the acceptable number of minutes of downtime to reach a given number of nines of availability.

At Google, however, a time-based metric for availability is usually not meaningful because we are looking across globally distributed services. Instead of using metrics around uptime, they define availability in terms of the request success rate:

```
               successful requests
availability = -------------------
                  total requests
```

Quantifying unplanned downtime as a request success rate also makes this availability metric more amenable for use in systems that do not typically serve end users directly.

### Risk Tolerance of Services

What does it mean to identify the risk tolerance of a service? In a formal environment or in the case of safety-critical systems, the risk tolerance of services is typically built directly into the basic product or service definition. At Google, services' risk tolerance tends to be less clearly defined.

To identify the risk tolerance of a service, SREs must work with the product owners to turn a set of business goals into explicit objectives to which we can engineer. In this case, the business goals we're concerned about have a direct impact on the performance and reliability of the service offered. In practice, this translation is easier said than done. While consumer services often have clear product owners, it is unusual for infrastructure services to have a similar structure of product ownership.

#### Identifying the Risk Tolerance of Consumer Services

When a product team exists, that team is usually the best resource to discuss the reliability requirements for a service. In the absence of a dedicated product team, the engineers building the system often play this role either knowingly or unknowingly.

There are many factors to consider when assessing the risk tolerance of services, such as the following:

- What level of availability is required?

- Do different types of failures have different effects on the service?

- How can we use the service cost to help locate a service on the risk continuum?

- What other service metrics are important to take into account?

**Target level of availability**

The target level of availability for a given Google service usually depends on the function it provides and how the service is positioned in the marketplace. The following list includes issues to consider:

- What level of service will the users expect?

- Does this service tie directly to revenue (either our revenue, or our customers' revenue)?

- Is this a paid service, or is it free?

- If there are competitors in the marketplace, what level of service do those competitors provide?

In this service targeted at consumers, or at enterprises?

**Types of failures**

The expected shape of failures for a given service is another important consideration. How resilient is our business to service downtime?

Which is worser for the service: A constant low rate of failures, or an occasional full-site outage? Both types of failure may result in the same absolute number of errors, but may have vastly different impacts on the business.

**Cost**

Cost is often the key factor in determining the appropriate availability target for a service. In determining the availability target for each service, we ask questions such as:

- If we were to build and operate these systems at one more nine of availability, what would our incremental increase in revenue be?

- Does this additional revenue offset the cost of reaching that level of reliability?

To make this trade-off equation more concrete, consider the following cost/benefit for an example service where ach request has equal value:

- **Proposed improvement in availability target:** 99.9% -> 99.99%

- **Proposed increase in availability:** 0.09%

- **Service revenue:** $1M

- **Value of improved availability:** $1M * 0.0009 = $900

In this case, if the cost of improving availability by one nine is less than $900, it is worth the investment. If the cost is greater than $900, the costs will exceed the projected increase in revenue.

It may be harder to set these targets when we do not have a simple translation function between reliability and revenue. One useful strategy may be to consider the background error rate of ISPs on the Internet. If failures are being measured from the end-user perspective and it is possible to drive the error rate for the service below the background error rate, those errors will fail within the noise for a given user's Internet connection.

**Other service metrics**

Examining the risk tolerance of services in relation to metrics besides availability is often fruitful. Understanding which metrics are important and which metrics aren't important provides us with degrees of freedom when attempting to take thoughtful risks.

#### Identifying the Risk Tolerance of Infrastructure Services

The requirements for building and running infrastructure components differ from the requirements for consumer products in a number of ways. A fundamental difference is that, by definition, infrastructure components have multiple clients, often with varying needs.

**Target level of availability**

Consider Bigtable, a massive-scale distributed storage system for structured data. Some consumer services serve data directly from Bigtable in the path of a user request.

Such services need low latency and high reliability. Other teams use Bigtable as a repository for data that they use to perform offline analysis on a regular basis. These teams tend to be more concerned about throughput than reliability. Risk tolerance for these two use cases is quite distinct.

One approach to meeting the needs of both use cases is to engineer all infrastructure services to be ultra-reliable. Given the fact that these infrastructure services also tend to aggregate huge amounts of resources, such an approach is usually far too expensive in practice. To understand the different needs of the different types of users, you can look at the desired state of the request queue for each type of Bigtable user.

**Types of failures**

The low-latency user wants Bigtable's request queues to be empty so that the system can process each outstanding request immediately upon arrival (Indeed, inefficient queuing is often a cause of high tail latency). The user concerned with offline analysis is more interested in system throughput, so that user wants request queues to never be empty. To optimize for throughput, the Big-table system should never need to idle while waiting for its next request.

As you can see, success and failure are antithetical for these sets of users. Success for the low-latency user is failure for the user concerned with offline analysis.

**Cost**

One way to satisfy these competing constraints in a cost-effective manner is to partition the infrastructure and offer it at multiple independent levels of services.

The key strategy with regards to infrastructure is to deliver services with explicitly delineated levels of service, thus enabling the clients to make the right risk and cost trade-offs when building their systems. With explicitly delineated levels of service, the infrastructure providers can effectively externalize the difference in the cost it takes to provide service at a given level to clients.

### Motivation for Error Budgets

Meanwhile, SRE performance is evaluated based upon reliability of a service, which implies an incentive to push back against a high rate of change. Information asymmetry between the two teams further amplifies this inherent tension. The product developers have more visibility into the time and effort involved in writing and releasing their code, while the SREs have more visibility into the service's reliability.

These tensions often reflect themselves in different opinions about the level of effort that should be put into engineering practices. The following list presents some typical tensions:

- **Software fault tolerance:** How hardened do we make the software to unexpected events? Too little, and we have a brittle, unusable product. Too much, and we have a product no one wants to use (but that runs very stably).

- **Testing:** Again, not enough testing and you have embarrassing outages, privacy data leaks, or a number of other press-worthy events. Too much testing, and you might lose your market.

- **Push frequency:** Every push is risky. How much should we work on reducing that risk, versus doing other work?

- **Canary duration and size:** It's a best practice to test a new release on some small subset of a typical workload, a practice often called canarying. How long do we wait, and how big it the canary?

**Forming Your Error Budget**

In order to base these decisions on objective data, the two teams jointly define a quarterly error budget based on the service's service level objective, or SLO. The error budget provides a clear, objective metric that determines how unreliable the service is allowed to be within a single quarter. This metric remover the politics from negotiations between the SREs and the product developers when deciding how much risk to allow.

Our practice is then as follows:

- Product Management defines an SLO, which sets an expectation of how much uptime the service should have per quarter.

- The actual uptime is measured by a neutral third party: our monitoring system.

- The difference between these two numbers is the "budget" of how much "unreliability" is remaining for the quarter.

- As long as the uptime measured is above the SLO - in other words, as long as there is error budget remaining - new releases can be pushed.

**Benefits**

The main benefit of an error budget is that it provides a common incentive that allows both product development and SRE to focus on finding the right balance between innovation and reliability.

Many products use this control loop to manage release velocity: as long as the system's SLOs are met, releases can continue. If SLO violations occur frequently enough to expend the error budget, releases are temporarily halted while additional resources are invested in system testing and development to make system more resilient, improve its performance, and so on.

More subtle and effective approaches are available than this sample on/off technique: for instance, slowing down releases or rolling them back when the SLO-violation error budget is close to being used up.

**Key Insights**

- Managing service reliability is largely about managing risk, and managing risk can be costly.

- 100% is probably never the right reliability target: not only is it impossible to achieve, it's typically more reliability than a service's users want or notice. Match the profile of the service to the risk the business is willing to take.

- An error budget aligns incentives and emphasizes joint ownership between SRE and product development. Error budgets make it easier to decide the rate of releases and to effectively defuse discussions about outages with stakeholders, and allows multiple teams to reach the same conclusion about production risk without rancor.

## Service Level Objectives

### Service Level Terminology

Terms SLI and SLO are also worth careful definition, because in common use, the term SLA is overloaded and has taken on a number of meanings depending on context.

#### Indicators

An SLI is a service level indicator - a carefully defined quantitative measure of some aspect of the level of service that is provided.

Most services consider request latency - how long it takes to return a response to a request - as a key SLI. Other common SLIs include the error rate, often expressed as a fraction of all requests received, and system throughput, typically measured in requests per second. The measurements are often aggregated: per example, raw data is collected over a measurement window and then turned into a rate, average, or percentile.

Ideally, the SLI directly measures a service level of interest, but sometimes only a proxy is available because the desired measure may be hard to obtain or interpret. For example, client-side latency is often the more user-relevant metric, but it might only be possible to measure latency at the server.

Another kind of SLI important to SREs is availability, or the fraction of the time that a service is usable. It is often defined in terms of the fraction of well-formed requests that succeed, sometimes called yield. (Durability - the likelihood that data will be retained over a long period of time - is equally important for data storage systems.)

#### Objectives

An SLO is a service level objective: a target value or range of values for a service level that is measured by an SLI.

A natural structure for SLOs is thus SLI ≤ target, or lower bound ≤ SLI ≤ upper bound.

Choosing an appropriate SLO is complex. To begin with, you don't always get to choose its value! For incoming HTTP requests from the outside world to your service, the queries per second (QPS) metric is essentially determined by desires of your users, and you can't really set an SLO for that.

On the other hand, you can say that you want the average latency per request to be under 100 milliseconds, and setting such a goal could in turn motivate you to write your frontend with low-latency behaviors of various kinds or to buy certain kinds of low-latency equipment (100 milliseconds is obviously an arbitrary value, but in general lower latency numbers are good. There are excellent reasons to believe that fast is better than slow, and that user-experienced latency above certain values actually drives people away).

Again, this is more subtle than it might at first appear, in that those two SLIs - QPS and latency - might be connected behind the scenes: higher QPS often leads to larger latencies, and it's common for services to have a performance cliff beyond some load threshold.

Choosing and publishing SLOs to users sets expectations about how a service will perform. This strategy can reduce unfounded complaints to service owners about, for example, the service being slow. Without an explicit SLO, users often develop their own beliefs about desired performance, which may be unrelated to the beliefs held by the people designing and operating the system. This dynamic can lead to both over-reliance on the service, when users incorrectly believe that a service will be more available than it actually is, and under-reliance, when prospective users believe a system is flakier and less reliable than it actually is.

#### Agreements

Finally, SLAs are service level agreements: an explicit or implicit contract with your users that includes consequences of meeting (or missing) the SLOs they contain. The consequences are most easily recognized when they are financial - a rebate or a penalty - but they can take other forms. An easy way to tell the difference between an SLO and an SLA is to ask "what happens if the SLOs aren't met?": if there is no explicit consequence, then you are almost certainly looking at an SLO.

SRE doesn't typically get involved in constructing SLAs, because are closely tied to business and product decisions. SRE does however, get involved in helping to avoid triggering the consequences of missed SLOs. They can also help to define the SLIs: there obviously needs to be an objective way to measure the SLOs in the agreement, or disagreements will arise.

### Indicators In Practice

How do you go about identifying what metrics are meaningful to your service or system?

#### What Do You And Your Users Care About?

You shouldn't use every metric you can track in your monitoring system as an SLI; an understanding of what your users want from the system will inform the judicious selection of a few indicators. Choosing too many indicators makes it hard to pay the right level of attention to the indicators that matter, while choosing too few may leave significant behaviors of your system unexamined.

We typically find that a handful of representative indicators are enough to evaluate and reason about a system's health.

Services ten to fall into a few broad categories in terms of the SLIs they find relevant:

- **User-facing serving systems:** Generally care about availability, latency, and throughput. In other words: Could we respond to the request? How long did it take to respond? How many requests could be handled?

- **Storage systems:** Often emphasize latency, availability, and durability. In other words: How long does it take to read or write data? Can we access the data on demand? Is the data still there when we need it?

- **Big data systems:** Such as data processing pipelines, tend to care about throughput and end-to-end latency. In other words: How much data is being processed? How long does it take the data to progress from ingestion to completion? (Some pipelines may also have targets for latency on individual processing stages.)

All systems should care about correctness: Was the right answer returned, the right data retrieved, the right analysis done? Correctness is important to track as an indicator of system health, even though it's often a property of the data in the system rather than the infrastructure per se, and so usually not an SRE responsibility to meet.

#### Collecting Indicators

Many indicator metrics are most naturally gathered on the server side, using a monitoring system such as Borgmon or Prometheus, or with a periodic log analysis - for instance, HTTP 500 responses as a fraction of all requests. However, some systems should be instrumented with client-side collection, because not measuring behavior at the client can miss a range of problems that affect users but don't affect server-side metrics.

#### Aggregation

For simplicity and usability, we often aggregate raw measurements. This needs to be done carefully.

Some metrics are seemingly straightforward, like the number of requests per second served, but even this apparently straightforward measurement implicitly aggregates data over the measurement window.

Most metrics are better thought of as distributions rather than averages. For example, for a latency SLI, some requests will be serviced quickly, while others will invariably take longer - sometimes much longer. A simple average can obscure these tail latencies, as well as changes in them.

Using percentiles for indicators allows you to consider the shape of the distribution and its differing attributes. The higher the variance in response times, the more the typical user experience is affected by long-tail behavior, an effect exacerbated at high load by queueing effects.

User studies have shown that people typically prefer a slightly slower system to one with high variance in response time, so some SRE teams focus only on high percentile values.

#### Standardize Indicators

We recommend that you standardize on common definitions for SLIs so that you don't have to reason about them from first principles each time. Any feature that conforms to the standard definition templates can be omitted from the specification of an individual SLI:

- Aggregation intervals: "Averaged over 1 minute"

- Aggregation regions: "All the tasks in a cluster"

- How frequently measurements are made: "Every 10 seconds"

- Which requests are included: "HTTP GETs from black-box monitoring jobs"

- How the data is acquired: "Through our monitoring, measured at the server"

- Data-access latency: "Time to last byte"

### Objectives in Practice

Start by thinking about what your users care about, not what you can measure. Often, what your users care about is difficult or impossible to measure, so you'll end up approximating users' needs in some way. However, if you simply start with what's easy to measure, you'll end up with less useful SLOs. As a result, we've sometimes found that working from desired objectives backward to specific indicators works better than choosing indicators and then coming up with targets;

#### Defining Objectives

For maximum clarity, SLOs should specify how they're measured and the conditions under which they're valid. For instance, we might say the following:

- 99% (averaged over 1 minute) of Get RPC calls will complete in less than 100ms (measured across all the backend servers).

- 99% of Get RPC calls will complete in less than 100ms.

If the shape of performance curves are important, the you can specify multiple SLO targets:

- 90% of Get RPC calls will complete in less than 1ms.

- 99% of Get RPC calls will complete in less than 10ms.

- 99.9% of Get RPC calls will complete in less than 100ms.

If you have users with heterogeneous workloads such as a bulk processing pipeline that cares about throughput and an interactive client that cares about latency, it may be appropriate to define separate objectives for each class of workload:

- 95% of throughput client's Set RPC calls will complete in < 1s.

- 99% of latency client's Set RPC calls with payloads < 1kb will complete in < 10ms.

It's both unrealistic and undesirable to insist that SLOs will be met 100% of the time: doing so can reduce the rate of innovation and deployment, require expensive, overly conservative solutions, or both. Instead, it is better to allow an error budget - a rate at which the SLOs can be missed - and track that on a daily or weekly basis.

The SLO violation rate can be compared against the error budget, with the gap used as an input to the process that decides when to roll out new releases.

#### Choosing Targets

Choosing targets (SLOs) is not a purely technical activity because of the product and business implications, which should be reflected in both SLIs and SLOs (and maybe SLAs) that are selected. Similarly, it may be necessary to trade off certain product attributes against others within the constraints posed by staffing, time to market, hardware availability, and funding. While SRE should be part of this conversation, and advise on the risks and viability of different options, we've learned a few lessons that can help make this a more productive discussion:

***Don't pick a target based on current performance***

While understanding the merits and limits of a system is essential, adopting values without reflection may lock you into supporting a system that requires heroic efforts to meet its targets, and that cannot be improved without significant redesign.

***Keep it simple***

Complicated aggregations in SLIs can obscure changes to system performance, and are also harder to reason about.

***Avoid absolutes***

While it's tempting to ask for a system that can scale its load "infinitely" without any latency increase and that is "always" available, this requirement is unrealistic. Even a system that approaches such ideals will probably take a long time to design and build, and will be expensive to operate - and probably turn out to be unnecessarily better than what users would be happy (or even delighted) to have.

***Have as few SLOs as possible***

Choose just enough SLOs to provide good coverage of your system's attributes. Defend the SLOs you pick: if you can't ever win a conversation about priorities by quoting a particular SLO, it's probably not worth having that SLO. However, not all product attributes are amenable to SLOs: it's hard to specify "user delight" with an SLO.

***Perfection can wait***

You can always refine SLO definitions and targets over time as you learn about a system's behavior. It's better to start with a loose target that you tighten than to choose an overly strict target that has to be relaxed when you discover it's unattainable.

SLOs can - and should - be a major driver in prioritizing work for SREs and product developers, because they reflect what users care about. A good SLO is a helpful, legitimate forcing function for a development team. But a poorly thought-out SLO can result in wasted work if a team uses heroic efforts to meet an overly aggressive SLO, or a bad product if the SLO is too lax. SLOs are a massive lever: use them wisely.

#### Control Measures

SLIs and SLOs are crucial elements in the control loops used to manage systems:

1. Monitor and measure the system's SLIs.

2. Compare the SLIs to the SLOs, and decide whether or not action is needed.

3. If action is needed, figure out what needs to happen in order to meet the target.

4. Take that action.

#### SLOs Set Expectations

Publishing SLOs sets expectations for system behavior. Users (and potential users) often want to know what they can expect from a service in order to understand whether it's appropriate for their use case.

In order to set realistic expectations for your users, you might consider using one or both of the following tactics:

***Keep a safety margin***

Using a tighter internal SLO than the SLO advertised to users give you room to respond to chronic problems before they become visible externally. An SLO buffer also makes it possible to accommodate reimplementations that trade performance for other attributes, such as cost or ease of maintainance, without having to disappoint users.

***Don't overachieve***

Users build on the reality of what you offer, rather than what you say you'll supply, particularly for infrastructure services. If your service's actual performance is much better than its stated SLO, users will come to rely on its current performance. You can avoid over-dependence by deliberately taking the system offline occasionally (Google's Chubby service introduced planned outages in response to being overly available), throttling some requests, or designing the system so that it isn't faster light loads.

Understanding how well a system is meeting its expectations helps decide whether to invest in making the system faster, more available, and more resilient. Alternatively, if the service is doing fine, perhaps staff time should be spent on other priorities, such as paying off technical debt, adding new features, or introducing other products.

### Agreements in Practice

Crafting an SLA requires business and legal teams to pick appropriate consequences and penalties for a breach. SRE's role is to help them understand the likelihood and difficulty of meeting the SLOs contained in the SLA. Much of the advice an SLO construction is also applicable for SLAs. It is wise to be conservative in what you advertise to users, as the broader the constituency, the harder it is to change or delete SLAs that prove yo be unwise or difficult to work with.

## Eliminating Toil

If a human operator needs to touch your system during normal operations, you have a bug. The definition of normal changes as your systems grow.

In SRE, we want to spend time on long-term engineering project work instead of operational work. Because the term operational work may be misinterpreted, we use a specific word: toil.

### Toil Defined

Toil is not just "work I don't like to do". It's also not simply equivalent to administrative chores or grungy work. Preferences as to what types of work are satisfying and enjoyable vary from person to person, and some people even enjoy manual, repetitive work.

There are also administrative chores that have to get done, but should not be categorized as toil: this is overhead. Overhead is often work not directly tied to running a production service, and includes tasks like team meetings, setting and grading goals, snippets, and HR paperwork.

Grungy work can sometimes have long-term value, and in that case, it's not toil, either. Cleaning up the entire alerting system configuration for your service and removing clutter may be grungy, but it's not toil.

So what is toil? Toil is the king of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows. Not every task deemed toil has all these attributes, but the more closely work matches one or more of the following descriptions, the more likely it is to be toil:

- **Manual:** This includes work such as manually running a script that automates some task. Running a script may be quicker than manually executing each step in the script, but the hands-on time a human spends running that script (not the elapsed time) is still toil time.

- **Repetitive:** If you're performing a task for the first time ever, or even the second time, this work is not toil. Toil is work you do over and over. If you're solving a novel problem or inventing a new solution, this work is not toil.

- **Automatable:** If a machine could accomplish the task just as well as a human, or the need for the task could be designed away, that task is toil. If human judgment is essential for the task, there's a good chance it's not toil.

- **Tactical:** Toil is interrupt-driven and reactive, rather than strategy-driven and proactive. Handling pager alerts is toil. We may never be able to eliminate this type of work completely, but we have to continually work toward minimizing it.

- **No enduring value:** If your service remains in the same state after you have finished a task, the task was probably toil. If the task produced a permanent improvement in your service, it probably wasn't toil, even if some amount of grunt work - such as digging into legacy code and configurations and straightening them out - was involved.

- **O(n) with service growth:** If the work involved in a task scales up linearly with service size, traffic volume, or user count, that task is probably toil. An ideally managed and designed service can grow by at least one order of magnitude with zero additional work, other than some one-time efforts to add resources.

### Why Less Toil Is Better

Feature development typically focuses on improving reliability, performance, or utilization, which often reduces toil as second-order effect.

Toil tends to expand if left unchecked and can quickly fill 100% of everyone's time. The work of reducing toil and scaling up services is the "Engineering" in Site Reliability Engineering. Engineering work is what enables the SRE organization to scale up sublinearly with service size and to manage services more efficiently than either a pure Dev team or a pure Ops team.

### What Qualifies as Engineering?

Engineering work is novel and intrinsically requires human judgment. It produces a permanent improvement in your service, and is guided by a strategy. It is frequently creative and innovative, taking a design-driven approach to solving a problem - the more generalized, the better. Engineering work helps your team or the SRE organization handle a larger service, or more services, with the same level of staffing.

Typical SRE activities fall into the following approximate categories:

- **Software engineering:** Involves writing or modifying code, in addition to any associated design and documentation work. Examples include writing automation scripts, creating tools or frameworks, adding service features for scalability and reliability, or modifying infrastructure code to make it more robust.

- **Systems engineering:** Involves configuring production systems, modifying configurations, or documenting systems in a way that produces lasting improvements from a one-time effort. Examples include monitoring setup and updates, load balancing configuration, server configuration, tunning of OS parameters, and load balances setup. Systems engineering also includes consulting on architecture, design, and productionization for developer teams.

- **Toil:** Work directly tied to running a service that is repetitive, manual, etc.

- **Overhead:** Administrative work not tied directly to running a service. Examples include hiring, HR paperwork, team/company meetings, bug queue hygiene, snippets, peer reviews and self-assessments, and training courses.

### Is Toil Always Bad?

Toil doesn't make everyone unhappy all the time, especially in small amounts. Predictable and repetitive tasks can be quite calming. They produce a sense of accomplishment and quick wins. They can be low-risk and low-stress activities. Some people gravitate toward tasks involving toil and may even enjoy they type of work.

Toil becomes toxic when experienced in large quantities. If you're burdened with too much toil, you should be very concerned and complain loudly. Among the many reasons why too much toil is bad, consider de following:

- **Career stagnation:** Your career progress will slow down or grind to a half it you spend too little time on projects.Google rewards grungy work when it's inevitable and has a big positive impact, but you can't make a career our of grunge.

- **Low morale:** People have different limits for how much toil they can tolerate, but everyone has a limit. Too much toil leads to burnout, boredom, and discontent.

Additionally, spending too much time on toil at the expense of time spent engineering hurst an SRE organization in the following ways:

- **Creates confusion:** We work hard to ensure that everyone who works in or with the SRE organization understands that we are an engineering organization. Individuals or teams within SRE that engage in too much toil undermine the clarity of that communication and confuse people about our role.

- **Slow progress:** Excessive toil makes a team less productive. A product's feature velocity will slow if the SRE team is too busy with manual work and firefighting to roll out new features promptly.

- **Sets precedent:** If you're too willing to take on toil, your Dev counterparts will have incentives to load you down with even more toil, sometimes whifting operational tasks that should rightfully be performed by Devs to SRE. Other teams may also start expecting SREs to take on such work, which is bad for obvious reasons.

- **Promotes attrition:** Even if you're not personally unhappy with toil, your current or future teammates might like it much less. If you build too much toil into your team's procedures, you motivate the team's best engineers to start looking elsewhere for a more rewarding job.

- **Causes breach of faith:** New hires or transfers who joined SRE with the promise of project work will feel cheated, which is bad for morale.

## Monitoring Distributed Systems

### Definitions

- **Monitoring:** Collecting, processing, aggregating, and displaying real-time quantitative data about a system, such as query counts and types, error counts and types, processing times, and server lifetimes.

- **White-box monitoring:** Monitoring based on metrics exposed by the internals of the system, including logs, interfaces like the Java Virtual Machine Profiling Interface, or an HTTP handler that emits internal statistics.

- **Black-box monitoring:** Testing externally visible behavior as a user would see it.

- **Dashboard:** An application (usually web-based) that provides a summary view of a service's core metrics. A dashboard may have filters, selectors, and so on, but is prebuilt to expose the metrics most important to its users. The dashboard might also display team information such as ticket queue length, a list of high-priority bugs, the current on-call engineer for a given area of responsibility, or recent pushes.

- **Alert:** A notification intended to be read by a human and that is pushed to a system such as a bug or ticket queue, an email alias, or a pager. Respectively, these alerts are classified as tickets, email alerts, and pages.

- **Root cause:** A defect in a software or human system that, if repaired, instills confidence that this event won't happen again in the same way. A given incident might have multiple root causes: for example, perhaps it was caused by a combination of insufficient process automation, software that crashed on bogus input, and insufficient testing of the script used to generate the configuration. Each of these factors might stand alone as a root cause, and each should be repaired.

- **Node and machine:** Used interchangeably to indicate a single instance of a running kernel in either a physical server, virtual machine, or container. There might be multiple services worth monitoring on a single machine. The services may either be:
  - Related to each other: for example, a caching server and a web server.
  - Unrelated services sharing software: for example, a code repository and a master for a configuration system like Puppet or Chef.

- **Push:** Any change to a service's running software or its configuration.

### Why Monitor?

There are many reasons to monitor a system, including:

- **Analysing long-term trends:** How big is my database and how fast is it growing? How quickly is my daily-active user count growing?

- **Comparing over time or experiment groups:** Are queries faster with Acme Bucket of Bytes 2.72 versus Ajax DB 3.14? How much better is my memcache hit rate with an extra node? Is my site slower than it was last week?

- **Alerting:** Something is broken, and somebody needs to fix it right now! Or, something might break soon, so somebody should look soon.

- **Building dashboards:** Dashboards should answer basic questions about your service, and normally include some form of the four golden signals.

- **Conducting ad hoc retrospective analysis (i.e., debugging):** Our latency just shot up; what else happened around the same time?

Monitoring and alerting enables a system to tell us when it's broken, or perhaps to tell us what's about to break. When the system ins't able to automatically fix itself, we want a human to investigate the alert, determine if there's a real problem at hand, mitigate the problem, and determine the root cause of the problem.

Paging a human is a quite expensive use of an employee's time. If an employee is at work, a page interrupts their workflow. If the employee is at home, a page interrupts their personal time, and perhaps even their sleep.

### Setting Reasonable Expectations for Monitoring

In general, Google has trended toward simpler and faster monitoring systems, with better tools for post hov analysis. We avoid "magic" systems that try to learn thresholds or automatically detect causality. Rules that detect unexpected changes in end-user request rates are one counterexample; while these rules are still kept as simple as possible, they give a very quick detection of a very simple, specific, severe anomaly. Other uses of monitoring data such as capacity planning and traffic prediction can tolerate more fragility, and thus, more complexity.

Rules that generate alerts for humans should be simple to understand and represent a clear failure.

### Symptoms Versus Causes

Your monitoring system should address two questions: what's broken, and why?

> **Symptom:** I'm serving HTTP 500s or 404s. / **Cause:** Database servers are refusing connections.
>
> **Symptom:** My responses are slow. / **Cause:** CPUs are overloaded by a bogosort, or an Ethernet cable is crimped under a rack, visible as partial packet loss.
>
> **Symptom:** Users in Antarctica aren't receiving animated cat GIFs. / **Cause:** Your Content Distribution Network hates scientists and feliness, and thus blacklisted some client IPs.
>
> **Symptom:** Private content is world-readable. / **Cause:** A new software push caused ACLs to be forgotten and allowed all requests.

"What" versus "why" is one of the most important distinctions in writing good monitoring with maximum signal and minimum noise.

### Black-Box Versus White-Box

We combine heavy use of white-box monitoring with modest but critical uses of black-box monitoring. The simplest way to think about black-box monitoring versus white-box monitoring is that black-box monitoring is symptom-oriented and represents active - not predicted - problems: "The system ins't working correctly, right now.". White-box monitoring depends on the ability to inspect the innards of the system, such as logs or HTTP endpoints, with instrumentation. White-box monitoring therefore allows detection of imminent problems, failures masked by retries, and so forth.

Note that in a multilayered system, one person's symptom is another person's cause. For example, suppose that a database's performance is slow. Slow database reads are a symptom for the database SRE who detects them. However, for the frontend SRE observing a slow website, the same slow database reads are a cause. Therefore, white-box monitoring is sometimes symptom-oriented, and sometimes cause-oriented, depending on just how informative your white-box is.

When collecting telemetry for debugging, white-box monitoring is essential. If web servers seem slow on database-heavy requests, you need to know both how fast the web server perceives the database to be, and how fast the database believes itself to be. Otherwise, you can't distinguish an actually slow database server from a network problem between your web server and your database.

### The Four Golden Signals

The four golden signals of monitoring are latency, traffic, errors, and saturation. If you can only measure four metrics of your user-facing system, focus on these four.

- **Latency:** The time it takes to service a request. It's important to distinguish between the latency of successful requests and the latency of failed requests. For example, an HTTP 500 error triggered due to loss of connection to a database or other critical backend might be served very quickly; however, as an HTTP 500 error indicates a failed request, factoring 500s into your overall latency might result in misleading calculations. On the other hand, a slow error is even worse than a fast error! Therefore, it's important to track error latency, as opposed to just filtering out errors.

- **Traffic:** A measure of how much demand is being placed on your system, measured in a high-level system-specific metric. For a web service, this measurement is usually HTTP requests per second, perhaps broken out by the nature of the requests (e.g., static versus dynamic content). For an audio streaming system, this measurement might focus on network I/O rate or concurrent sessions. For a key-value storage system, this measurement might be transactions and retrievals per second.

- **Errors:** The rate of requests that fail, either explicitly (e.g. HTTP 500s), implicitly (for example, an HTTP 200 success response, but coupled with the wrong content), or by policy (for example, "If you committed to one-second response times, any request over one second is an error"). Where protocol response codes are insufficient to express all failure conditions, secondary (internal) protocols may be necessary to track partial failure modes. Monitoring these cases can be drastically different: catching HTTP 500s at your load balancer can do a decent job of catching all completely failed requests, while only end-to-end system tests can detect that you're serving the wrong content.

- **Saturation:** How "full" your service is. A measure of your system fraction, emphasizing the resources that are most constrained (e.g., in a memory-constrained system, show memory; in an I/O-constrained system, show I/O). Note that many systems degrade in performance before they achieve 100% utilization, so having a utilization target is essential. In complex systems, saturation can be supplemented with higher-level load measurement: can your service properly handle double the traffic, handle only 10% more traffic, or handle even less traffic than it currently receives? For very simple services that have no parameters that alter the complexity of the request (e.g., "Give me a nonce" or "I need a globally unique monotonic integer") that rarely change configuration, a static value from a load test might be adequate. As discussed in the previous paragraph, however, most services need to use indirect signals like CPU utilization or network bandwidth that have a known upper bound. Latency increases are often a leading indicator of saturation. Measuring your 99th percentile response time over some small window (e.g., one minute) can give a very early signal of saturation. Finally, saturation is also concerned with predictions of impending saturation, such as "It looks like your database will fill its hard drive in 4 hours."

### Worrying About Your Tail (or, Instrumentation and Performance)

When building a monitoring system from scratch, it's tempting to design a system based upon the mean of some quantity: the mean latency, the mean CPU usage of your nodes, or the mean fullness of your databases. The danger presented by the latter two cases is obvious: CPUs and databases can easily be utilized in a very imbalanced way. The same holds for latency. If you run a web service with an average latency of 100ms at 1000 requests per second, 1% of requests might easily take 5 seconds.

The simplest way to differentiate between a low average and a very slow "tail" of requests is to collect request counts bucketed by latencies (suitable for rendering a histogram), rather than actual latencies: how many requests did I serve that took between 0ms and 10ms, between 10ms and 30ms, between 30ms and 100ms, between 100ms and 300ms, and so on? Distributing the histogram boundaries approximately exponentially is often an easy way to visualize the distribution of your requests.

### Choosing An Appropriate Resolution for Measurements

Different aspects of a system would be measured with different levels of granularity. For example:

- Observing CPU load over the time span of a minute won't reveal even quite long-lived spikes that drive high tail latencies.

- On the other hand, for a web service targeting no more than 9 hours aggregate downtime per year (99.9% annual uptime), probing for a 200 (success) status more than once or twice a minute is probably unnecessarily frequent.

- Similarly, checking hard drive fullness for a service targeting 99.9% availability more than once every 1-2 minutes is probably unnecessary.

Take care in how you structure the granularity of your measurements. Collecting per-second measurements of CPU load might yield interesting data, but such frequent measurements may be very expensive to collect, store, and analyze.

1. Record the current CPU utilization each second.

2. Using buckets of 5% granularity, increment the appropriate CPU utilization bucket each second.

3. Aggregate those values every minute.

### As Simple as Possible, No Simpler

Piling all these requirements on top of each other can add up to a very complex monitoring system - your system might end up with the following levels of complexity.

- Alerts on different latency thresholds, at different percentiles, on all kinds of different metrics.

- Extra code to detect and expose possible causes.

- Associated dashboards for each of these possible causes.

The sources of potential complexity are never-ending. Like all software systems, monitoring can become so complex that it's fragile, complicated to change, and a maintenance burden.

Therefore, design your monitoring system with an eye toward simplicity. In choosing what to monitor, keep the following guidelines in mind:

- The rules that catch real incidents most often should be as simple, predictable, and reliable as possible.

- Data collection, aggregation, and alerting configuration that is rarely exercised (e.g., less than once a quarter for some SRE teams) should be up for removal.

- Signals that are collected, but not exposed in any pre-baked dashboard nor used by any alert, are candidates for removal.

### Trying These Principles Together

When creating rules for monitoring and alerting, asking the following questions can help you avoid false positives and pager burnout:

- Does this rule detect an otherwise undetected condition that is urgent, actionable, and actively or imminently user-visible?

- Will I ever be able to ignore this alert, knowing it's benign? When and why will I be able to ignore this alert, and how can I avoid this scenario?

- Does this alert definitely indicate that users are being negatively affected? Are there detectable cases in which users aren't being negatively impacted, such as drained traffic or test deployments, that should be filtered out?

- Can I take action in response to this alert? Is that action urgent, or could it wait until morning? Could the action be safely automated? Will that action be along-term fix, or just a short-term workaround?

- Are other people getting paged for this issue, therefore rendering at least one of the pages unnecessary?

These questions reflect a fundamental philosophy on pages and pagers:

- Every time the pager goes off, I should be able to react with a sense of urgency. I can only react with a sense of urgency a few times a day before I become fatigued.

- Every page should be actionable.

- Every page response should require intelligence. If a page merely merits a robotic response, it shouldn't be a page.

- Pages should be about a novel problem or an event that hasn't been seen before.

## Release Engineering

Running reliable services requires reliable release processes. Changes to any aspect of the release process should be intentional, rather than accidental. SREs care about this process from source code to deployment.

### The Role of a Release Engineer

Release engineers define best practices for using our tools in order to make sure projects are released using consistent and repeatable methodologies. Examples include compiler flags, formats for build identification tags, and required steps during a build.

### Philosophy

Release engineering is guided by an engineering and service philosophy that's expressed through four major principles.

#### Self-Service Model

In order to work at scale, teams must be self-sufficient. Release engineering has developed best practices and tools that allow our product development teams to control and run their own release processes.

Although we have thousand of engineers and products, we can achieve a high release velocity because individual teams can decide how often and when to release new versions of their products.

Release processes can be automated to the point that they require minimal involvement by the engineers, and many projects are automatically built and released using a combination of our automated build system and our deployment tools. Releases are truly automatic, and only require engineer involvement if and when problems arise.

#### High Velocity

User-facing software is rebuilt frequently, as we aim to roll out customer-facing features as quickly as possible.

We have embraced the philosophy that frequent releases result in fewer changes between versions. This approach makes testing and troubleshooting easier. Some teams perform hourly builds and then select the version to actually deploy to production from the resulting pool of builds.

Selection is based upon the test results and the features contained in a given build.

#### Hermetic Builds

Build tools must allow us to ensure consistency and repeatability. If two people attempt to build the same revision number in the source code repository on different machines, we expect identical results. Our builds are hermetic, meaning that they are insensitive to the libraries and other software installed on the build machine. Instead, builds depend on known versions of build tools, such as compilers, and dependencies, such as libraries.

The build process if self-contained and must not rely on services that are external to the build environment.

Rebuilding older releases when we need to fix a bug in software that's running in production can be a challenge. We accomplish this task by rebuilding at the same revision as the original build and including specific changes that were submitted after that point in time. We call this tactic cherry picking. Our build tools are themselves versioned based on the revision in the source code repository for the project being built. Therefore, a project built last month won't use this month's version of the compiler if a cherry pick is required, because that version may contain incompatible or undesired features.

#### Enforcement of Policies and Procedures

Several layers of security and access control determine who can perform specific operations when releasing a project. Gated operations include:

- Approving source code changes - this operation is managed through configuration files scattered throughout the codebase.

- Specifying the actions to be performed during the release process.

- Creating a new release.

- Approving the initial integration proposal (which is a request to perform a build at a specific revision number in the source code repository) and subsequent cherry picks.

- Deploying a new release.

- Making changes to a project's build configuration.

Almost all changes to the codebase require a code review, which is a steam-lined action integrated into our normal developer workflow. Our automated release system produces a report of all changes contained in a release, which is archived with other build artifacts. By allowing SREs to understand what changes are included ina  new release of a project, this report can expedite troubleshooting when there are problems with a release.

### Continuous Build and Deployment

Google has developed an automated release system called Rapid. Rapid is a system that leverages a number of Google technologies to provide a framework that delivers scalable, hermetic, and reliable releases.

#### Building

Blaze is Google's build tool of choice. It support building binaries from a range of languages. When performing a build, Blaze automatically builds the dependency targets.

Build targets for binaries and unit tests are defined in Rapid's project configuration files. Project-specific flags, such as a unique build identifier, are passed by Rapid to Blaze. All binaries support a flag that displays the build date, the revision number, and the build identifier, which allow us to easily associate a binary to a record of how it was built.

#### Branching

All code is checked into the main branch of the source code tree (mainline). However, most major projects don't release directly from the mainline. Instead, we branch from the mainline at a specific version and never merge changes from the branch into the mainline. Bug fixes are submitted to the mainline and then cherry picked into the branch for inclusion in the release. This practice avoids inadvertently picking up unrelated changes submitted to the mainline since the original build occurred. Using this branch and cherry pick method, we know the exact contents of each release.

#### Testing

A continuous test system runs unit tests against the code in the mainline each time a change is submitted, allowing us to detect build and test failures quickly. Release engineering recommends that the continuous build test targets correspond to the same test targets that gate the project release. We also recommend creating releases at the revision number (version) of the last continuous test build that successfully completed all tests. These measures decrease the change that subsequent changes made to the mainline will cause failures during the build performed at release time.

During the release process, we re-run the unit tests using the release branch and create an audit trail showing that all the tests passed. This step is important because if a release involves cherry picks, the release branch may contain a version of the code that doesn't exist anywhere on the mainline. We want to guarantee that the tests pass in the context of what's actually being released.

To complement the continuous test system, we use an independent testing environment that runs system-level tests on packaged build artifacts. These tests can be launched manually or from Rapid.

#### Packaging

Software is distributed to our production machines via the Midas Package Manager. MPM assembles packages based on Blaze rules that list the build artifacts to include, along with their owners and permissions. Packages are named (search, shakespeare, frontend, etc.), versioned with a unique hash, and signed to ensure authenticity. MPM support applying labels to a particular version of a package. Rapid applies a label containing the build ID, which guarantees that a package can be uniquely referenced using the name of the package and this label.

Labels can be applied to an MPM package to indicate a package's location in the release process (dev, canary, or production). If you apply an existing label to a package, the label is automatically moved from the old package to the new package. For example: if a package is labeled as canary, someone subsequently installing the canary version of that package will automatically receive the newest version of the package with the label canary.

#### Rapid

Rapid is configured with files called blueprints. Blueprints are written in an internal configuration language and are used to define build and test targets, rules for deployment, and administrative information (like project owners). Role-based access control lists determine who can perform specific actions on a Rapid project.

Each Rapid project has workflows that define the actions to perform during the release process. Workflow actions can be performed serially or in parallel, and a workflow can launch other workflows. Rapid dispatches work requests to tasks running as a Borg job on our production servers. Because Rapid uses our production infrastructure, it can handle thousand of release requests simultaneously.

A typical release process proceeds as follows:

1. Rapid uses the requested integration revision number (often obtained automatically from our continuous test system) to create a release branch.

2. Rapid uses Blaze to compile all the binaries and execute unit tests, often performing these two steps in parallel. Compilation and testing occur in environments dedicated to those specific tasks, as opposed to taking place in the Bord jog where the Rapid workflow is executing. This separation allows us to parallelize work easily.

3. Build artifacts are then available for system testing and canary deployments. A typical canary deployment involves starting a few jobs in our production environment after the completion of system tests.

4. The results of  each step of the process are logged. A report of all changes since the last release is created.

Rapid allows us to manage our release branches and cherry picks; individual cherry pick requests can be approved or rejected for inclusion in a release.

#### Deployment

Rapid is often used to drive simple deployments directly. It updates the Borg jobs to use newly built MPM packages based on deployment definitions in the blueprint files and specialized task executors.

For more complicated deployments, we use Sisyphus, which is a general-purpose rollout automation framework developed by SRE. A rollout is a logical unit of work that is composed of one or more individual tasks. Sisyphus provides a set of Python classes than can be extended to support any deployment process. It has a dashboard that allows for finer control on how the rollout is performed and provides a way to monitor the rollout's progress.

In a typical integration, Rapid creates a rollout in a long-running Sisyphus job. Rapid knows the build label associated with the MPM package it created, and can specify that build label when creating the rollout in Sisyphus. Sisyphus uses the build label to specify which version of MPM packages should be deployed.

With sisyphus, the rollout process can be as simple or complicated as necessary. For example, it can update all the associated jobs immediately or it can roll out a new binary to successive clusters over a period of several hours. Our goal is to fit the deployment process to the risk profile of a given service.

In development or pre-production environments, we may build hourly and push releases automatically when all tests pass. For large user-facing services, we may push by starting in one cluster and expand exponentially until all clusters are updated. For sensitive pieces of infrastructure, we may extend the rollout over several days, interleaving them across instances in different geographic regions.

### Configuration Management

Configuration management is one area of particularly close collaboration between release engineers and SREs. Although configuration management may initially seem a deceptively simple problem, configuration changes are a potential source of instability. As a result, our approach to releasing and managing system and service configurations has evolved substantially over time. Today we use several models for distributing configuration files. All schemes involve storing configuration in our primary source code repository and enforcing a strict code review requirement.

#### Use the mainline for configuration

This was the first method used to configure services in Bord (and the systems that pre-dated Borg). Using this scheme, developers and SREs modify configuration files at the head of the main branch. The changes are reviewed and then applied to the running system. As a result, binary releases and configuration changes are decoupled. While conceptually and procedurally simple, this technique often leads to skew between the checked-in version of the configuration files and the running version of the configuration file because jobs must be updated in order to pick up the changes.

#### Include configuration files and binaries in the same MPM package

For projects with few configuration files or projects where the files (or a subset of files) change with each release cycle, the configuration files can be included in the MPM package with the binaries. While this strategy limits flexibility by binding the binary and configuration files tightly, it simplifies deployment, because it only requires installing one package.

#### Package configuration files into MPM "configuration packages"

We can apply the hermetic principle to configuration management. Binary configurations tend to be tightly bound to particular versions of binaries, so we leverage the build and packaging systems to snapshot and release configuration files alongside their binaries, we can use the build ID to reconstruct the configuration at a specific point in time.

#### Read configuration files from an external store

Some projects have configuration files that need to change frequently or dynamically (while the binary is running). These files can be stored in Chubby Bigtable, or our source-based filesystem. In summary, project owners consider the different options for distributing and managing configuration files and decide which works best on a case-by-case basis.

## Simplicity

The price of reliability is the pursuit of the utmost simplicity. Software systems are inherently dynamic and unstable. A software system can only be perfectly stable if it exists in a vacuum. If we stop changing the codebase, we stop introducing bugs. If the underlying hardware or libraries never change, neither of these components will introduce bugs. If we freeze the current user base, we'll never have to scale the system. In fact, a good summary of the SRE approach to managing systems is: "At the end of the day, our job is to keep agility and stability in balance in the system".

### System Stability Versus Agility

It sometimes makes sense to sacrifice stability for the sake of agility. Code that comes with an expiration date can be much more liberal with test coverage and release management because it will never be shipped to production or be seen by users.

For the majority of production software systems, we want a balanced mix of stability and agility. SREs work to create procedures, practices, and tools that render software more reliable. At the same time, SREs ensure that this work has as little impact on developer agility as possible.

In fact, SRE's experience has found that reliable processes tend to actually increase developer agility: rapid, reliable production rollouts make changes in production easier to see. As a result, once a bug surfaces, it takes less time to find and fix that bug. Building reliability into development allows developers to focus their attention on what we really do care about - the functionality and performance of their software and systems.

### The Virtue of Boring

Unlike just about everything else in life, "boring" is actually a positive attribute when it comes to software! We don't want our programs to be spontaneous and interesting; we want them to stick to the script and predictably accomplish their business goals.

It is very important to consider the difference between essential complexity and accidental complexity. Essential complexity is the complexity inherent in a given situation that cannot be removed from a problem definition, whereas accidental complexity is more fluid and can be resolved with engineering effort. For example, writing a web server entails dealing with the essential complexity of serving web pages quickly. However, if we write a web server in Java, we may introduce accidental complexity when trying to minimize the performance impact of garbage collection.

With an eye towards minimizing accidental complexity, SRE teams should:

- Push back when accidental complexity is introduced into the systems for which they are responsible.

- Constantly strive to eliminate complexity in systems they onboard and for which they assume operational responsibility.

### Minimal APIs

"Perfection is finally attained not when there is no longer more to add, but when there is no longer anything to take away". This principle is also applicable to the design and construction of software. APIs are a particularly clear expression of why this rule should be followed.

Writing clear, minimal APIs is an essential aspect of managing simplicity in a software system. The fewer methods and arguments we provide to consumers of the API, the easier that API will be to understand, and the more effort we can devote to making those methods as good as they can possibly be. Again, a recurring theme appears: the conscious decision to not take on certain problems allows us to focus on our code problem and make the solutions we explicitly set out to create substantially better. In software, less is more! A small simple API is usually also a hallmark of a well-understood problem.

### Modularity

The ability to make changes to parts of the system in isolation is essential to creating a supportable system. Specifically, loose coupling between binaries, or between binaries and configuration, is a simplicity pattern that simultaneously promotes developer agility and system stability. If a bug is discovered in one program that is a component of a larger system, that bug can be fixed and pushed to production independent of the rest of the system.

As a system grows more complex, the separation of responsibility between APIs and between binaries becomes increasingly important.

### Release Simplicity

Simples releases are generally better than complicated releases. It is much easier to measure and understand the impact of a single change rather than a batch of changes released simultaneously. If we release 100 unrelated changes to a system at the same time and performance gets worse, understanding which changes impacted performance, and how they did so, will take considerable effort or additional instrumentation.

If the release is performed in smaller batches, we can move faster with more confidence because each code change can be understood in isolation in the larger system.

## Being On-Call

Being on-call is a critical duty that many operations and engineering teams must undertake in order to keep their services reliable and available.

Several professions require employees to perform some sort on-call duty, which entails being available for calls during both working and nonworking hours.

The SRE teams are quite different from purely operational teams in that they place heavy emphasis on the use of engieering to approach problems.

### Life of an On-Call Engineer

As the guardians of production systems, on-call engineers take care of their assigned operations by managing outages that affect the team and performing and/or vetting production changes.

When on-call, an engineer is available to perform operations on production systems within minutes, according to the paging response times agreed to by the team and the business system owners. Typical values are 5 minutes for user-facing or otherwise highly time-critical services, and 30 minutes for less time-sensitive systems.

As soon as a page is received and acknowledged, the on-call engineer is expected to triage the problem and work toward its resolution, possibly involving other team members and escalating as needed.

### Balanced On-Call

SRE teams have specific constraints on the quantity of on-call shifts. The quantity of on-call can be calculated by the percent of time spent by engineers on on-call duties.The quality of on-call can be calculated by the number of incidents that occur during an on-call shift.

If a service entails enough work to justify growing a single-site team, we prefer to create a multi-site team. A multi-site team is advantageous for two reasons:

- Night shifts have detrimental effects on people's health, and a multi-site "follow the sun" rotation allows teams to avoid night shifts altogether.

- Limiting the number of engineers in the on-call rotation ensures that engineers do not lose touch with the production systems.

However, multi-site teams incur communication and coordination overhead. Therefore, the decision to go multi-site or single-site should be based upon the tradeoffs each option entails, the importance of the system, and the workload each system generates.

#### Compesation

Adequate compensation neds to be considered for out-of-hours support. Different organizations handle on-call compensation in different ways; Google offers time-off-in-lieu or straight cash compensation, capped at some propostion of overall salary.

The compesation cap represents, in practice, a limit on the amount of on-call work that will be taken on by any individual.

### Feeling Safe

Modern research identifies two distinct ways of thinking that an individual may, consciously or subconsciously, choose when faced with challenges:

- Intuitive, automatic, and rapid action.

- Rational, focused, and deliberate cognitive functions.

When one is dealing with the outages related to complex systems, the second of these options is more likely to produce better results and lead to well-planned incident handling.

Heuristics are very tempting behaviors when one is one-call. For example, when the same alert pages for the fourth time in the week, and the previous three pages were initiated by an external infrastructure system, it is extremely tempting to exercise confirmation bias by automatically associating this fourth occurence of the problem with the previous cause.

While intuition and quick reactions can seem like desirable traits in the middle of incident management, they have downsides. Intuition can be wrong and is often less supportable by obvious data. Thus, following intuition can lead an engineer to waste time pursuing a line of reasoning that is incorrect from the start.

It's important that on-call SREs understand that they can rely on several resources that make the experience of being on-call less daunting than it may seem. The most important on-call resources are:

- Clear escalation paths.

- Well-defined incident-management procedures.

- A blameless postmortem culture.

## Effective Troubleshooting

> "Be warned that being an expert is more than understanding how a system is supposed to work. Expertise is gained by investigating why a system doesn't work."
>
> "Ways in which things go right are special cases of the ways in which things go wrong"

Troubleshooting is a critical skill for anyone who operates distributed computing systems - especially SREs - but it's often viewed as an innate skill that some people have and others don't. One reason for this assumption is that, for those who troubleshoot often, it's an ingrained process; explaining how to troubleshoot is difficult, much like explaining how to ride a bike. However, we believe that troubleshooting is both learnable and teachable.

Novices are often tripped up when troubleshooting because the exercise ideally depends upon two factors: an understanding of how to troubleshoot generically (per example, without any particular system knowledge) and a solid knowledge of the system. While you can investigate a problem using only the generic process and derivation from first principles, we usually find this approach to be less efficient and less effective than understanding how things are supposed to work. Knowledge of the system typically limits the effectiveness of an SRE new to a system; there's little substitute to learning how the system is designed and built.

### Theory

Formally, we can think of the troubleshooting process as an application of the hypothetico-deductive method: given a set of observations about a system and a theoretical basis for understanding system behavior, we iteratively hypothesize potential causes for the failure and try to test those hypotheses.

We'd start with a problem report telling us that something is wrong with the system. Then we can look at the system's telemetry and logs to understand its current state. This information, combined with our knowledge of how the system is built, how it should operate, and its failure modes, enables us to identify some possible causes.

> Problem Report -> Triage -> Examine -> Diagnose -> Test/Treat -> Cure

We can then test our hypotheses in one of two ways. We can compare the observed state of the system against our theories to find confirming or disconfirming evidence. Or, in some cases, we can actively "treat" the system - that is, change the system in a controlled way - and observe the results. This second approach refines our understanding of the system's state and possible cause(s) of the reported problems. Using either of these strategies, we repeatedly test until a root cause is identified, at which point we can then take corrective action to prevent a recurrence and write a postmortem. Of course, fixing the proximate cause(s) needn't always wait for root-causing or portmortem writing.

### Common Pitfalls

Ineffective troubleshooting sessions are plagued by problems at the Triage, Examine, and Diagnose steps, often because of a lack of deep system understanding. The following are common pitfalls to avoid:

- Looking at symptoms that aren't relevant or misunderstanding the meaning of system metrics. Wild goose chases often result.

- Misunderstanding how to change the system, its inputs, or its environment, so as to safely and effectively test hypotheses.

- Coming up with wildly improbable theories about what's wrong, or latching on to cause of past problems, reasoning that since it happened once, it must be happening again.

- Hunting down spurious correlations that are actually coincidences or are correlated with shared causes.

The third trap is a set of logical fallacies that can be avoided by remembering that correlation is not causation. As systems grow in size and complexity and as more metrics are monitored, it's inevitable that there will be events that happen to correlate well with other events, purely by coincidence.

Understanding failured in our reasoning process is the first step to avoiding them and becoming more effective in solving problems. A methodical approach to knowing what we do know, what we don't know, and what we need to know, makes it simpler and more straightforward to figure out what's gone wrong and how to fix it.

### In Practice

In practice, of course, troubleshooting is never as clean as our idealized model suggests it should be.There are some steps that can make the process less painful and more productive for both those experiencing system problems and those responding to them.

#### Problem Report

Every problem starts with a problem report, which might be an automated alert or one of your colleagues saying, "The system is slow". An effective report should tell you the expected behavior, the actual behavior, and, if possible, how to reproduce the behavior.

Ideally, the reports should have a consistent form and be stored in a searchable location, such as a bug tracking system. This may also be a good point at which to provide tools for problem reporters to try self-diagnosing or self-repairing common issues on their own.

#### Triage

Once you receive a problem report, the next step is to figure out what to do about it. Problems can vary in severity: an issue might affect only one user under very specific circumstances (and might have a workaround), or it might entail a complete global outage for a service.

Your response should be appropriate for the problem's impact: it's appropriate to declare an all-hands-on-deck emergency for the latter, but doing so for the former is overkill. Assessing an issue's severity requires an exercise of good engineering judgment and, often, a degree of calm under pressure.

Your first response in a major outage may be to start troubleshooting and try to find a root cause as quickly as possible. Ignore that instinct!

Instead, your couse of action should be to make the system work as well as it can under the circumstances. This may entail emergency options, such as diverting traffic from a broken cluster to others that are still working, dropping traffic wholesale to prevent a cascading failure, or disabling subsystems to lighten the load. Stopping the bleeding should be your first priority; you aren't helping your users if the system dies while you're root-causing. Of course, an emphasis on rapid triage doesn't preclude taking steps to preserve evidence of what's going wrong, such as logs, to help with subsequent root-cause analysis.

Novice pilots are taught that their first responsibility in an emergency is to fly the airplane; troubleshooting is secondary to getting the plane and everyone on it safely onto the ground. This approach is also applicable to computer systems: for example, if a bug is leading to possible unrecoverable data corruption, freezing the system to prevent further failure may be better than letting this behavior continue.

#### Examine

We need to be able to examine what each component in the system is doing in order to understand whether or not it's behaving correctly.

Ideally, a monitoring system is recording metrics for your system. These metrics are a good place to start figuring out what's wrong.

Logging is another invaluable tool. Exporting information about each operation and about system state makes it possible to understand exactly what a process was doing at a given point in time. You may need to analyze system logs across one or many processes.

Tracing requests through the whole stack provides a very powerful way to understand how a distributed system is working, though varying use cases imply significantly different tracing designs.

#### Diagnose

A thorough understanding of the system's design is decidedly helpful for coming up with plausible hypotheses about what's gone wrong, but there are also some generic practices that will help even without domain knowledge.

Ideally, components in a system have well-defined interfaces and perform known transformations from their input to their output. It's then possible to look at the connections between components - or, equivalently, at the data flowing between them - to determine whether a given component is working properly.

Having a solid reproducible test case makes debugging much faster, and it may be possible to use the case in a non-production environment where more invasive or riskier techniques are available than would be possible in production.

A malfunctioning system is often still trying to do something - just not the thing you want it to be doing. Finding out what it's doing, then asking why it's doing that and where its resources are being used or where its output is going can help you understand how things have gone wrong.

Systems have inertia: we've found that a working computer system tends to remain in motion until acted upon by an external force, such as a configuration change or a shift in the type of load served. Recent changes to a system can be a productive place to start identifying what's going wrong.

While the generic tools described previously are helpful across a broad range of problem domains, you will likely find it helpful to build tools and systems to help with diagnosing your particular services.

#### Test and Treat

Once you've come up with a short list of possible causes, it's time to try to find which factor is at the root of the actual problem. Using the experimental method, we can try to tule in or rule out our hypotheses.

For instance, suppose we think a problem is caused by either a network failure between an application logic server and a database server, or by the database refusing connections. Trying to connect to the database with the same credentials the application logic server uses can refute the second hypothesis, while pinging the database server may be able to refute the first, depending on network topology, firewall rules, and other factors. Following the code and trying to imitate the code flow, step-by-step, may point exactly what's going wrong.

There are a number of considerations to keep in mind when designing tests:

- An ideal test should have mutually exclusive alternatives, so that it can rule one group of hypotheses in and rule another set out. In practice, this may be difficuld to achieve.

- Consider the obvious first: perform the tests in decreasing order of likelihood, considering possible risks to the system from the test.

- An experiment may provide misleading results due to confounding factors.

- Active tests may have side effects that change future test results.

Take clear notes of what ideas you had, which tests you ran, and the results you saw. Particularly when you are dealing with more complicated and drawn-out cases, this documentation may be crucial in helping you remember exactly what happened and prevent having to repeat these steps.

#### Cure

Ideally, you've now narrowed the set of possible causes to one. Next, we'd like to prove that it's the actual cause. Definitively proving that a given factor caused a problem - by reproducing it at will - can be difficult to do in production systems; often, we can only find probable causal factors, for the following reasons:

- **Systems are complex.** It's quite likely that there are multiple factors, each of which individually is not the cause, but which taken jointly are causes. Real systems are also often path-dependent, so that they must be in a specific state before a failure occurs.

- **Reproducing the problem in a live production system may not be an option,** either because of the complexity of getting the system into a state where the failure can be triggered, or because further downtime may be unacceptable. Having a nonproduction environment can mitigate these challenges, though at the cost of having another copy of the system to run.

Once you've found the factors that caused the problem, it's time to write up notes on what went wrong with the system, how you tracked down the problem, how you fixed the problem, and how to provent it from happening again. In other words, you need to write a postmortem.

### Negative Results Are Magic

A "negative" result is an experimental outcome in which the expected effect is absent - that is, any experiment that doesn't work out as planned. This includes new designs, heruistics, or human processes that fail yo improve upon the systems they replace.

- **Negative results should not be ignored or discounted:** Realizing you're wrong has much value; a clear negative result can resolve some of the hardest design questions. Often a team has two seemingly reasonable designs but process in one direction has to address vague and speculative questions about whether the other direction might be better.

- **Experiments with negative results are conclusive:** They tell us something certain about production, or the design space, or the performance limits of an existing system. They can help others determine whether their own experiments or designs are worthwhile.

- **Tools and methods can outlive the experiment and inform future work:** As an example, benchmarking tools and load generators can result just as easily from disconforming experiment as a supporting one.

- **Publishing negative results improves our industry's data-driven culture:** Accounting for negative results and statistical insignificance reduces the bias in our metrics and provides an example to others of how to maturely accept uncertainty. By publishing everything, you encourage others to do the same, and everyone in the industry collectively learns much more quickly.

- **Publish your results:** If you are interested in an experiment's results, there's a good chance that other people are as well. When you publish the results, those people do not have to design and run a similar experiment themselves. It's tempting and common to avoid reporting negative results because it's easy to perceive that the experiment "failed". Some experiments are doomed, and they tend to be caught by review. Many more experiments are simply unreported because people mistakenly believe that negative results are not progress.

## Emergency Response

Things break,; that's life.

Regardless of the stakes involved or the size of an organization, one trait that's vital to the long-term health of an organization, and that consequently sets that organization apart from others, is how the people involved respond to an emergency.

Few of us naturally respond well during an emergency. A proper response takes preparation and periodic, pertinent, hands-on training. Establishing and maintaining thorough training and testing processes requires the support of the board and management, in addition to the careful attention of staff.

### What to Do When Systems Break

First of all, don't panic! You aren't alone, and the sky isn't falling. You're a professional and trained to handle this sort of situation. Typically, no one is in physical danger - only those poor electrons are in peril. At the very worst, half of the internet is down. So take a deep breath...and carry on.

If you feel overwhelmed, pull in more people. Sometimes it may even be necessary to page the entire company.

### Induced Emergency

In Google they try to force the system to break by something in order to learn and improve the systems.


- **Test-Induced Emergency:** SRE break their systems, watch how they fail, and make changes to improve reliability and prevent the failures from recurring.

- **Change-Induced Emergency:** Perform numerous tests on configuration changes to make sure they don't result in unexpected and undesired behavior.

- **Process-Induced Emergency:** Sometimes, the efficiency of automations can be a bit frightening when things do not go quite according to plan.

That way, you can answer some questions such as: Details, Response, Findings (What went well, What we learned).

### All Problems Have Solutions

Time and experience have shown that systems will not only break, but will break in ways that one could never previously imagine. One of the greatest lessons Google has learned is that a solution exists, even if it may not be obvious, especially to the person whose pager is screaming. If you can't think of a solution, cast your net farther. Involve more of your teammates, seek help, do whatever you have to do, but do it quickly. The highest priority is to resolve the issue at hand quickly. Oftentimes, the person with the most state is the one whose actions somehow triggered the event. Utilize that person.

### Learn from the Past. Don't Repeat It.

#### Keep a History of Outages

There is no better way to learn than to document what has broken in the past. History is about learning from everyone's mistakes. Be thorough, be honest, but most of all, ask hard questions. Look for specific actions that might prevent such an outage from recurring, not just tactically, but also strategically. Ensure that everyone within the company can learn what you have learned by publishing and organizing postmortems.

#### Ask the Big, Even Improbable, Questions: What If...?

There is no greater test than reality. Ask yourself some big, open-ended questions. What if the building power fails...? What if the network equipmant racks are standing in two feet of water...? What if the primary datacenter suddenly goes dark...?

What do you do? Who do you call? Who will write the check? Do you have a plan? Do you know how to react? Do you know how your systems will react? Could you minimize the impact if it were to happen now? Could the person sitting next to you do the same?

#### Encourage Proactive Testing

When it comes to failures, theory and reality are two very different realms. Until your system has actually failed, you don't truly know how that system, its dependent systems, or your users will react. Don't rely on assumptions or what you can't or haven't tested.

## Managing Incidents

Effective incident management is key to limiting the disruption caused by an incident and restoring normal business operations as quickly as possible. If you haven't gamed out your response to potential incidents in advance, principled incident management can go out the window in real-life situations.

### Elements of Incident Management Process

Incident management skills and practices exist to channel the energies of enthusiastic individuals.

#### Recursive Separation of Responsabilities

It''s important to make sure that everybody involved in the incident knows their role and doesn't stray onto someone else's turf. Somewhat counterintuitively, a clear separation of responsibilities allows individuals more autonomy than they might otherwise have, since they need not second-guess their colleagues.

If the load on a given member becomes excessive, that person needs to ask the planning lead for more staff. They should then delegate work to others, a task that might entail creating subincidents. Alternatively, a role leader might delegate system components to colleagues, who report high-level information back up to the leaders.

Several distinct roles should be delegated to particular individuals:

- **Incident Command:** The incident commander holds the high-level state about the incident. They structure the incident response task force, assigning responsibilities according to need and priority. De facto, the commander holds all positions that they have not delegated. If appropriate, they can remove roadblocks that prevent Ops from working most effectively.

- **Operational Work:** The Ops lead works with the incident commander to respond to the incident by applying operational tools to the task at hand. The operations team should be the only group modifying the system during an incident.

- **Communication:** This person is the public face of the incident response task force. Their duties most definitely include issuing periodic updates to the incident response team and stakeholders (usually via email), and may extend to tasks such as keeping the incident document accurate and up to date.

- **Planning:** The planning role supports Ops by dealing with longer-term issues, such as filing bugs, ordering dinner, arranging handoffs, and tracking how the system has diverged from the norm so it can be reverted once the incident is resolved.

## Postmortem Culture: Learning from Failure

As SREs, we work with large-scale, complex, distributed systems. We constantly enchance our services with new features and add new systems. Incidents and outages are inevitable given our scale and velocity of change. When an incident occurs, we fix the underlying issue, and services return to their normal operation conditions. Unless we have some formaliezd process of learning from these incidents in place, they may recur ad infinitum. Left unchecked, incidents can multiply in complexity or even cascade, overwhelming a system and its operators and ultimately impacting our users.

The postmortem concept is well known in the technology industry. A postmortem is a written record of an incident, its impact, the actions taken to mitigate or resolve it, the root cause(s) and the follow up actions to prevent the incident from recurring.

Teams have some internal flexibility, but common postmortem triggers include:

- User-visible downtime or degradation beyond a certain threshold.

- Data loss of any kind.

- On-call engineer intervention (release rollback, rerouting of traffic, etc.).

- A resolution time above some threshold.

- A monitoring failure (which usually implies manual incident discovery).

It is important to define postmortem criteria before an incident occurs so that everyone knows when a postmortem is necessary. In addition to these objective triggers, any stakeholder may request a postmortem for an event.

Blameless postmortems are a tenet of SRE culture. For a postmortem to be truly blameless it must focus on identifying the contributing causes of the incident without indicting any individual or team for bad or inappropriate behavior.

## Tracking Outages

Improving reliability over time is only possible if you start from a known baseline and can track progress.

### Aggregation

A single event may, and often will, trigger multiple alerts. For example, network failures cause timeouts and unreachable backend services fore everyone, so all affected teams receive their own alerts, including the owners of backend services; meanwhile, the network operations center will have its own klaxons ringing.

However, even smaller issues affecting a single service may trigger multiple alerts due to multiple error conditions being diagnosed.

While it is worthwhile to attempt to minimize the number of alerts triggered by a single event, triggering multiple alerts is unavoidable in most trade-off calculations between false positives and false negatives.

The ability to group multiple alerts together into a single incident is critical n dealing with this duplication. Sending an email saying "this is the same thing as that other thing; they are symptoms of the same incident" works for a given alert: it can prevent duplication of debugging or panic.

### Tagging

Of course, not every alerting event is an incident. False-positive alerts occur, as well as test events or mistargeted emails from humans.

Tagging is used to add metadata to notifications, at any level. Tags are mostly free-form, single "words". Colons, however, are interpreted as semantic separators, which subtly promotes the use of hierarchical namespaces and allows some automatic treatment.

This namespacing is supported by suggested tag prefixes, primarily "cause" and "action", but the list is team-specific and generated based on historical usage.

### Analysis

Of course, SRE does much more than just react to incidents. Historical data is useful when one is responding to an incident - the question "what did we do last time?" is always a good starting point. But historical information is far more useful when it concerns systemic, periodic, or other wider problems that may exist. Enabling such analysis is one of the most important functions of an outage tracking tool.

The bottom layer of analysis encompasses counting and basic aggregate statistics for reporting. The details depend on the team, but include information such as incidents per week/month/quarter and alerts per incident.

The next layer is more important, and easy to provide: comparison between teams/services and over tme to identify first patterns and trends. This layer allows teams to determine whether a given alert load is "normal" relative to their own track record and that of other services. "That's the third time this week" can be good or bad, but knowing whether "it" used to happen five times per day or five times per month allows interpretation.

The next step in data analysis is finding wider issues, which are not just raw counts but require some semantic analysis. For example, identifying the infrastructure component causing most incidents, and therefore the potential benefit from increasing the stability or performance of this component, assumes that there is a straightforward way to provide this information alongside the incident records.

### Unexpected Benefits

Being able to identify that an alert, or flood of alerts, coincides with a given other outage has obvious benefits: it increases the speed of diagnosis and reduces load on other teams by acknowledging that there is indeed an incident. there are additional nonobvious benefits.

Improved cross-team visibility can and does make a big difference in incident resolution, or at least in incident mitigation.

## Testing for Reliability

One key responsibility of Site Reliability Engineers is to quantify confidence in systems they maintain. SREs perform this task by adapting classical software testing techniques to system at scale. Confidence can be measured both by past reliability and future reliability.

The former is captured by analyzing data provided by monitoring historic system behavior, while the latter is quantified by making predictions from data about past system behavior.

In order for these predictions to be strong enough to be useful, one of the following conditions must hold:

- The site remains completely unchanged over time with no software releases or changes in the server fleet, which means that future behavior will be similar to past behavior.

- You can confidently describe all changes to the site, in order for analysis to allow for the uncertainty incurred by each of these changes.

Testing is a mechanism you use to demonstrate specific areas of equivalence when changes occur. Each test that passes both before and after a change reduces the uncertainty for which the analysis needs to allow.

The amount of testing you need to conduct depends on the reliability requirements for your system. As the percentage of your codebase covered by tests increases, you reduce uncertainty and the potential decrease in reliability from each change.

### Types of Software Testing

Software tests broadly fall into two categories: traditional and production. Traditional tests are more common in software development to evaluate the correctness of software offline, during development. Production tests are performed on a live web service to evaluate whether a deployed software system is working correctly.

#### Traditional Tests

- **Unit tests:** A unit test is the smallest and simplest form of software testing. These tests are employed to assess a separable unit of software, such as a class or function, for correctness independent of the larger software system that contains the unit. Unit tests are also employed as a form of specification to ensure that a function or module exactly performs the behavior required by the system. Unit tests are commonly used to introduce test-driven development concepts.

- **Integration tests:** Software components that pass individual unit tests are assembled into larger components. Engineers that run an integration test on an assembled component to verify that it functions correctly.

- **System tests:** A system test is the largest scale test that engineers run for an undeployed system. All modules belonging to a specific component, such as a server that passed integration tests, are assembled into the system. Then the engineer tests the end-to-end functionality of the system. System tests come in many different flavors:

  - **Smoke tests:** Smoke tests, in which engineers test very simple but critical behavior, are among the simplest type of system tests. Smoke tests are also known as sanity testing, and serve to short-circuit additional and more expensive testing.

  - **Performance tests:** Once basic correctness is established via a smoke test, a common next step is to write another variant of a system test to ensure that the performance of the system stays acceptable over the duration of its lifecycle.

  - **Regression tests:** Another type of system test involves preventing bugs from sneaking back into the codebase. Regression tests can be analogized to a gallery of rogue bugs that historically caused the system to fail or produce incorrect results. By documenting these bugs as tests at the system or integration level, engineers refactoring the codebase can be sure that they don't accidentally introduce bugs that they've already invested time and effort to eliminate.

It is important to note that tests have a cost, both in terms of time and computational resources. At one extreme, unit tests are very cheap in both dimensions, as they can usually be completed in milliseconds on the resources available on a laptop. At the other end of the spectrum, bringing up a complete server with required dependencies (or mock equivalents) to run related tests can take significantly more time - from several minutes to multiple hours - and possibly require dedicated computing resources. Mindfulness of these costs is essential to developer productivity, and also encourages more efficient use of testing resources.

#### Production Tests

- **Configuration test:** Configuration tests are built and tested for a specific version of the checked-in configuration file. Comparing which version of the test is passing in relation to the goal version for automation implicitly indicates how far actual production currently lags being ongoing engineering work. Nonhermetic configuration tests tend to be especially valuable as part of a distributed monitoring solution since the pattern of passes/fails across production can identify paths through the service stack that don't have sensible combinations of the local configurations.

- **Stress test:** In order to safely operate a system, SREs need to understand the limits of both the system and its components. In many cases, individual components don't gracefully degrade beyond a certain point - instead, they catastrophically fail. Engineers use stress tests to find the limits on a web service. Stress tests answer questions such as:

  - How full can a database get before writes start to fail?

  - How many queries a second can be sent to an application server before it becomes overloaded, causing requests to fail?

- **Canary test:** The canary test is conspicuously absent from this list of production tests. The term canary comes from the phrase "canary in a coal mine", and refers to the practice of using a live bird to detect toxic gases before humans were poisoned. A canary test isn't really a test; rather, it's structured user acceptance. Whereas configuration and stress tests confirm the existence of a specific condition over deterministic software, a canary test is more ad hoc. It only exposes the code under test to less predictable live production traffic.

<!--- Current Page 327 / Last Page 327 -->