# Site Reliability Engineering

## Introduction

### The Sysadmin Approach to Service Management

Historically, companies have employed systems administrators to run complex computing systems.

The sysadmin model of service management has several advantages. For companies deciding how to run and staff a service, this approach is relatively easy to implement: as a familiar industry paradigm, there are many examples from which to learn and emulate. An array of existing tools, software components, and integration companies are available to help run those assembled systems, so a novice sysadmin team doesn't have to reinvent the wheel and design a system from scratch.

The sysadmin approach and the accompanying development/ops split has a number of disadvantages and pitfalls. These fall broadly into two categories: direct costs and indirect costs.

- Direct costs are neither subtle nor ambiguous. Running a service with a team that relies on manual intervention for both change management and event handling becomes expensive as the service and/or traffic to the service grows, because the size of the team necessarily scales with the load generated by the system.

- The indirect costs of the development/ops split can be subtle, but are often more expensive to the organization than the direct costs. These costs arise from the fact that the two teams are quite different in background, skill set, and incentives. They use different vocabulary to describe situations; they carry different assumptions about both risk and possibilities for technical solutions; they have different assumptions about the target level of product stability.

Traditional operations teams and their counterparts in product development thus often end up in conflict, most visibly over how quickly software can be released to production. At their core, the development teams want to launch new features and see them adopted by users. At their core, the ops teams want to make sure the service doesn't break while they are holding the pager. Because most outages are caused by some king of change - a new configuration, a new feature launch, or a new type of user traffic - the two teams' goals are fundamentally in tension.

The ops team attempts to safeguard the running system against the risk of change by introducing launch and change gates.

### Google's Approach to Srevice Management: Site Reliability Engineering

Conflict isn't an inevitable part of offering a software service. Google has chosen to run their systems with a different approach: Their Site Reliability Engineering teams focus on hiring software engineers to run their products and to create systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.

SRE is what happens when you ask a software engineer to design an operations team. The result of Google's approach to hiring for SRE is that whey end up with a team of people who will quickly become bored by performing tasks by hand, and have the skill set necessary to write software to replace their previously manual work, even when the solution is complicated.

By design, it is crucical that SRE teams are focused on engineering. Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload. Google places a 50% cap on the aggregate "ops" work for all SREs - tickes, on-call, manual tasks, etc. This cap ensures that the SRE team has enough time in their schedule to make the service stable and operable. This cap is an upper bound; over time, left to their own devices, the SRE team should end up with very little operational load and almost entirely engage in development tasks, because the service basically runs and repairs itself: we want systems that are automatic, not just automated. In practice, scale and new features keep SREs on their toes.

Google's rule of thumb is that an SRE team must spend the remaining 50% of its time actually doing development.

Google SRE's approach to running large-scale systems has many advantages. Because SREs are directly modifying code in their pursuit of making Google's systems run themselves, SRE teams are characterized by both rapid innovation and a large acceptance of change. The number of SREs needed to run, maintain, and improve a system scales sublinearly with the size of the system. Finally, not only does SRE circumbent the dysfunctionality of the dev/ops split, but this structure also improves our product development teams: easy transfers between product development and SRE teams cross-train the entire group, and improve skills of developers who otherwise may have difficulty learning how to build a million-core distributed system.

> Devops or SRE? One could view DevOps as a generalization of several core SRE principles to a wider range of organizations, management structures, and personnel. One could equivalently view SRE as a specific implementation of DevOps with some idiosyncratic extensions.

### Tenets of SRE

In general, an SRE team is responsible for the availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning of their services.

#### Ensuring a Durable Focus on Engineering

In practice, this is accomplished by monitoring the amount of operational work being done by SREs, and redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, (re)integrating developers into on-call pager rotations, and so on.

When they are focused on operations work, on average, SREs should receive a maximum of two events per 8-12-hour on-call shift. This target volume gives the on-call engineer enough time to handle the event accurately and quickly, clean up and restore normal service, and then conduct a postmortem.

Postmortems should be written for all significant incidents, regardless of whether or not they pages; portmortems that did not trigger a page are even more valuable, as they likely point to clear monitoring gaps. This investigation should establish what happened in detail, find all root causes of the event, and assign actions to correct the problem or improve how it is addressed next time. Google operates under a blame-free postmortem culture, with a goal of exposing faults and applying engineering to fix these faults, rather than avoiding of minimizing them.

#### Pursuing Maximum Change Velocity Without Violating a Service's SLO

The error budget stems from the observation that 100% is the wrong reliability target for basically everything (pacemakers and ati-lock brakes being notable exceptions). In general, for any software service or system, 100% is not the right reliability target because no user can tell the difference between a system being 100% available and 99.999% available. There are many other systems in the path between user and service (their laptop, their home WiFi, their ISP, the power grid...) and those systems collectively are far less than 99.999% available. Thus, the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.

What, then, is the right reliability target for the system? This actually isn't a technical question at all - it's a product question, which should take the following considerations into account:

- What level of availability will the users be happy with, given how they use the product?

- What alternatives are available to users who are dissatisfied with the product's availability?

- What happens to user's usage of the product at different availability levels?

The business or the product must establish the system's availability target. Once that target is established, the error budget is one minus the availability target. A service that's 99.99% available is 0.01% unavailable. That permitted 0.01% unavailability is the service's error budget. We can spend the budget on anything we want, as long as we don't overspend it.

So how to we want to spend the error budget? The development team wants to launch features and attract new users. Ideally, we would spend all of our error budget taking risks with things we launch in order to launch them quickly. This basic premise describes the whole model of error budgets. As soon as SRE activities are conceptualized in this framework, freeing up the error budget through tactics such as phased wollouts and 1% experiments can optimize for quicker launches.

The use of an error budget resolves the structural conflict of incentives between development and SRE. SRE's goal is no longer "zero outages"; rather, SREs and product developers aim to spend the error budget getting maximum feature velocity. This change makes all the difference. An outage is no longer a "bad" thing - it is an expected part of the process of innovation, and an occurence that both development and SRE teams manage rather than fear.

#### Monitoring

Monitoring is one of the primary means by which service owners keep track of a system's health and availability. As such, monitoring strategy should be constructed thoughtfully. A classic and common approach to monitoring is to watch for a specific value or condition, and then to trigger an email alert when that value is exceeded or that condition occurs. However, this type of email alerting is not an effective solution: a system that requires a human to read an email and decide whether or not some type of action needs to be taken in response is fundamentally flawed. Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.


There are three kinds of valid monitoring output:

- **Alerts:** Signify that a human needs to take action immediately in response to something that is either happening or about to happen, in order to improve the situation.

- **Tickets:** Signify that a human needs to take action, but not immediately. The system cannot automatically handle the situation, but if a human takes action in a few days, no damage will result.

- **Logging:** No one needs to look at this information, but it is recorded for diagnostic or forensic purposes. The expectation is that no one reads logs unless something else prompts them to do so.

#### Emergency Response

Humans add latency. Even if a given system experiences more actual failures, a system that can avoid emergencise that require human intervention will have higher  availability than a system that requires hands-on intevention. When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a "playbook" produces roughly a 3x improvement in MTTR as compared to the strategy of "winging it".

The hero jack-of-all-trades on-call engineer does work, but the practiced on-call engineer armed with a playbook works much better.

While no playbook, no matter how comprehensive it may be, is a substitute for smart engineers able to think on the fly, clear and through troubleshooting steps and tips are valuable when responding to a high-stakes or time-sensitive page. Thus, Google SRE relies on on-call playbooks, in addition to exercisse such as the "Wheel of Misfortune", to prepare engineers to react to on-call events.

#### Change Management

SRE has found that roughly 70% of outages are due to changes in a live system. Best practices in this domain use automation to accomplish the following:

- Implementing progressive rollouts.

- Quickly and accurately detecting problems.

- Rolling back changes safely when problems arise.

By removing humans from the loop, these practices avoid the normal problems of fatigue, familiarity/contempt, and inattention to highly repetitive tasks. As a result, both release velocity and safety increase.

#### Demand Forecasting and Capacity Planning

It can be viewed as ensuring that there is sufficient capacity and redundancy to serve projected future deman with the required availability. There's nothing particularly special about these concepts, except that a surprising number of services and teams don't take the steps necessary to ensure that the required capacity is in place by the time it is needed.

Capacity planning should take both organic growth  (which stems from natural product adoption and usage by customers) and inorganic growth (which results from events like feature launches, marketing campaigns, or other business-driven changes) into account.

Several steps are mandatory in capacity planning:

- An accurate organic demand forecast which extends beyond the lead time required for acquiring capacity.

- An accurate incorporation of inorganic demand sources into the demand forecast.

- Regular load testing of the system to correlate raw capacity (servers, disks, and so on) to service capacity.

Because capacity is critical to availability, it naturally follows that SRE team must be in charge of capacity planning, which means they also must be in charge of provisioning.

#### Provisioning

Provisioning combines both change management and capacity planning. In our experience, provisioning must be conducted quickly and only when necessary, as capacity is expensive.

This exercise must also be done correctly or capacity often involves spinning up a new instance or location, making significant modification to existing systems (configuration files, load balancers, networking), and validating that the new capacity performs and delivers correct results.

#### Efficiency and Performance

Efficient use of resource is important any time a service cares about money. Because SRE ultimately controls provisioning, it must also be involved in any work on utilization, as utilization is a function of how a given service works and how it is provisioned. It follows that paying close attentions to the provisioning strategy for a service, and therefore its utilization, provides a very, very big lever on the service's total costs.

Resource use is a function of demand (load), capacity, and software efficiency. SREs predict demand, provision capacity, and can modify the software. These three factors are a large part (though not the entirety) or a service's efficiency.

Software systems become slower as load is added to them. A slowdown in a service equates to a loss of capacity. At some point, a slowing system stops serving, which corresponds to infinite slowness. SREs provision to meet a capacity target at a specific response speed, and thus are keenly interested in a service's performance. SREs and product developers will (and should) monitor and modify a service to improve its performance, thus adding capacity and improving efficiency.


<!--- Current Page 58 / Last Page 58 -->