# Site Reliability Engineering

## Introduction

### The Sysadmin Approach to Service Management

Historically, companies have employed systems administrators to run complex computing systems.

The sysadmin model of service management has several advantages. For companies deciding how to run and staff a service, this approach is relatively easy to implement: as a familiar industry paradigm, there are many examples from which to learn and emulate. An array of existing tools, software components, and integration companies are available to help run those assembled systems, so a novice sysadmin team doesn't have to reinvent the wheel and design a system from scratch.

The sysadmin approach and the accompanying development/ops split has a number of disadvantages and pitfalls. These fall broadly into two categories: direct costs and indirect costs.

- Direct costs are neither subtle nor ambiguous. Running a service with a team that relies on manual intervention for both change management and event handling becomes expensive as the service and/or traffic to the service grows, because the size of the team necessarily scales with the load generated by the system.

- The indirect costs of the development/ops split can be subtle, but are often more expensive to the organization than the direct costs. These costs arise from the fact that the two teams are quite different in background, skill set, and incentives. They use different vocabulary to describe situations; they carry different assumptions about both risk and possibilities for technical solutions; they have different assumptions about the target level of product stability.

Traditional operations teams and their counterparts in product development thus often end up in conflict, most visibly over how quickly software can be released to production. At their core, the development teams want to launch new features and see them adopted by users. At their core, the ops teams want to make sure the service doesn't break while they are holding the pager. Because most outages are caused by some king of change - a new configuration, a new feature launch, or a new type of user traffic - the two teams' goals are fundamentally in tension.

The ops team attempts to safeguard the running system against the risk of change by introducing launch and change gates.

### Google's Approach to Srevice Management: Site Reliability Engineering

Conflict isn't an inevitable part of offering a software service. Google has chosen to run their systems with a different approach: Their Site Reliability Engineering teams focus on hiring software engineers to run their products and to create systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.

SRE is what happens when you ask a software engineer to design an operations team. The result of Google's approach to hiring for SRE is that whey end up with a team of people who will quickly become bored by performing tasks by hand, and have the skill set necessary to write software to replace their previously manual work, even when the solution is complicated.

By design, it is crucical that SRE teams are focused on engineering. Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload. Google places a 50% cap on the aggregate "ops" work for all SREs - tickes, on-call, manual tasks, etc. This cap ensures that the SRE team has enough time in their schedule to make the service stable and operable. This cap is an upper bound; over time, left to their own devices, the SRE team should end up with very little operational load and almost entirely engage in development tasks, because the service basically runs and repairs itself: we want systems that are automatic, not just automated. In practice, scale and new features keep SREs on their toes.

Google's rule of thumb is that an SRE team must spend the remaining 50% of its time actually doing development.

Google SRE's approach to running large-scale systems has many advantages. Because SREs are directly modifying code in their pursuit of making Google's systems run themselves, SRE teams are characterized by both rapid innovation and a large acceptance of change. The number of SREs needed to run, maintain, and improve a system scales sublinearly with the size of the system. Finally, not only does SRE circumbent the dysfunctionality of the dev/ops split, but this structure also improves our product development teams: easy transfers between product development and SRE teams cross-train the entire group, and improve skills of developers who otherwise may have difficulty learning how to build a million-core distributed system.

> Devops or SRE? One could view DevOps as a generalization of several core SRE principles to a wider range of organizations, management structures, and personnel. One could equivalently view SRE as a specific implementation of DevOps with some idiosyncratic extensions.

### Tenets of SRE

In general, an SRE team is responsible for the availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning of their services.

#### Ensuring a Durable Focus on Engineering

In practice, this is accomplished by monitoring the amount of operational work being done by SREs, and redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, (re)integrating developers into on-call pager rotations, and so on.

When they are focused on operations work, on average, SREs should receive a maximum of two events per 8-12-hour on-call shift. This target volume gives the on-call engineer enough time to handle the event accurately and quickly, clean up and restore normal service, and then conduct a postmortem.

Postmortems should be written for all significant incidents, regardless of whether or not they pages; portmortems that did not trigger a page are even more valuable, as they likely point to clear monitoring gaps. This investigation should establish what happened in detail, find all root causes of the event, and assign actions to correct the problem or improve how it is addressed next time. Google operates under a blame-free postmortem culture, with a goal of exposing faults and applying engineering to fix these faults, rather than avoiding of minimizing them.

#### Pursuing Maximum Change Velocity Without Violating a Service's SLO

The error budget stems from the observation that 100% is the wrong reliability target for basically everything (pacemakers and ati-lock brakes being notable exceptions). In general, for any software service or system, 100% is not the right reliability target because no user can tell the difference between a system being 100% available and 99.999% available. There are many other systems in the path between user and service (their laptop, their home WiFi, their ISP, the power grid...) and those systems collectively are far less than 99.999% available. Thus, the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.

What, then, is the right reliability target for the system? This actually isn't a technical question at all - it's a product question, which should take the following considerations into account:

- What level of availability will the users be happy with, given how they use the product?

- What alternatives are available to users who are dissatisfied with the product's availability?

- What happens to user's usage of the product at different availability levels?

The business or the product must establish the system's availability target. Once that target is established, the error budget is one minus the availability target. A service that's 99.99% available is 0.01% unavailable. That permitted 0.01% unavailability is the service's error budget. We can spend the budget on anything we want, as long as we don't overspend it.

So how to we want to spend the error budget? The development team wants to launch features and attract new users. Ideally, we would spend all of our error budget taking risks with things we launch in order to launch them quickly. This basic premise describes the whole model of error budgets. As soon as SRE activities are conceptualized in this framework, freeing up the error budget through tactics such as phased wollouts and 1% experiments can optimize for quicker launches.

The use of an error budget resolves the structural conflict of incentives between development and SRE. SRE's goal is no longer "zero outages"; rather, SREs and product developers aim to spend the error budget getting maximum feature velocity. This change makes all the difference. An outage is no longer a "bad" thing - it is an expected part of the process of innovation, and an occurence that both development and SRE teams manage rather than fear.

#### Monitoring

Monitoring is one of the primary means by which service owners keep track of a system's health and availability. As such, monitoring strategy should be constructed thoughtfully. A classic and common approach to monitoring is to watch for a specific value or condition, and then to trigger an email alert when that value is exceeded or that condition occurs. However, this type of email alerting is not an effective solution: a system that requires a human to read an email and decide whether or not some type of action needs to be taken in response is fundamentally flawed. Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.


There are three kinds of valid monitoring output:

- **Alerts:** Signify that a human needs to take action immediately in response to something that is either happening or about to happen, in order to improve the situation.

- **Tickets:** Signify that a human needs to take action, but not immediately. The system cannot automatically handle the situation, but if a human takes action in a few days, no damage will result.

- **Logging:** No one needs to look at this information, but it is recorded for diagnostic or forensic purposes. The expectation is that no one reads logs unless something else prompts them to do so.

#### Emergency Response

Humans add latency. Even if a given system experiences more actual failures, a system that can avoid emergencise that require human intervention will have higher  availability than a system that requires hands-on intevention. When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a "playbook" produces roughly a 3x improvement in MTTR as compared to the strategy of "winging it".

The hero jack-of-all-trades on-call engineer does work, but the practiced on-call engineer armed with a playbook works much better.

While no playbook, no matter how comprehensive it may be, is a substitute for smart engineers able to think on the fly, clear and through troubleshooting steps and tips are valuable when responding to a high-stakes or time-sensitive page. Thus, Google SRE relies on on-call playbooks, in addition to exercisse such as the "Wheel of Misfortune", to prepare engineers to react to on-call events.

#### Change Management

SRE has found that roughly 70% of outages are due to changes in a live system. Best practices in this domain use automation to accomplish the following:

- Implementing progressive rollouts.

- Quickly and accurately detecting problems.

- Rolling back changes safely when problems arise.

By removing humans from the loop, these practices avoid the normal problems of fatigue, familiarity/contempt, and inattention to highly repetitive tasks. As a result, both release velocity and safety increase.

#### Demand Forecasting and Capacity Planning

It can be viewed as ensuring that there is sufficient capacity and redundancy to serve projected future deman with the required availability. There's nothing particularly special about these concepts, except that a surprising number of services and teams don't take the steps necessary to ensure that the required capacity is in place by the time it is needed.

Capacity planning should take both organic growth  (which stems from natural product adoption and usage by customers) and inorganic growth (which results from events like feature launches, marketing campaigns, or other business-driven changes) into account.

Several steps are mandatory in capacity planning:

- An accurate organic demand forecast which extends beyond the lead time required for acquiring capacity.

- An accurate incorporation of inorganic demand sources into the demand forecast.

- Regular load testing of the system to correlate raw capacity (servers, disks, and so on) to service capacity.

Because capacity is critical to availability, it naturally follows that SRE team must be in charge of capacity planning, which means they also must be in charge of provisioning.

#### Provisioning

Provisioning combines both change management and capacity planning. In our experience, provisioning must be conducted quickly and only when necessary, as capacity is expensive.

This exercise must also be done correctly or capacity often involves spinning up a new instance or location, making significant modification to existing systems (configuration files, load balancers, networking), and validating that the new capacity performs and delivers correct results.

#### Efficiency and Performance

Efficient use of resource is important any time a service cares about money. Because SRE ultimately controls provisioning, it must also be involved in any work on utilization, as utilization is a function of how a given service works and how it is provisioned. It follows that paying close attentions to the provisioning strategy for a service, and therefore its utilization, provides a very, very big lever on the service's total costs.

Resource use is a function of demand (load), capacity, and software efficiency. SREs predict demand, provision capacity, and can modify the software. These three factors are a large part (though not the entirety) or a service's efficiency.

Software systems become slower as load is added to them. A slowdown in a service equates to a loss of capacity. At some point, a slowing system stops serving, which corresponds to infinite slowness. SREs provision to meet a capacity target at a specific response speed, and thus are keenly interested in a service's performance. SREs and product developers will (and should) monitor and modify a service to improve its performance, thus adding capacity and improving efficiency.

### Managing Risk

Unreliable systems can quickly errode users' confidence, so we want to reduce the change of system failure. However, experience shows that as we build systems, cost does not increase linearly as reliability increments - an incremental improvement in reliability may cost 100x more than the previous increment.

The costliness has two dimensions:

- **The cost of redundant machine/compute resources:** The cost associated with redudant equipment that, for example, allows us to take systems offline for routine or unforeseen maintenance, or provides space for us to store parity code blocks that provide a minimum data durability guarantee.

- **The opportunity cost:** The cost borne by an organization when it allocates engineering resources to build systems or features that diminish risk instead of features that are directly visible to or usable by end users. These engineers no longer work on new features and products for end users.

In SRE, we manage service reliability largely by managing risk. We conceptualize risk as continuum.  In a sense, we view the availability target as both a minimum and a maximum. The key advantage of this framing is that it unlocks explicit, thoughtful risktaking.

### Measuring Service Risk

By setting a target, we can assess our current performance and track improvements or degradations over time. For service risk, it is not immediately clear ho to reduce all of the potential factors into a single metric. Service failures can havemany potential effects, including user dissatisfaction, harm, or loss of trust; direct or indirect revenue loss; brand or reputational impact; and undesirable press coverage.

Clearly, some of these factors are very hard to measure. To make this problem tractable and consistent across many types of systems we run, we focus on unplanned downtime.

For most services, the most straightforward way of representing risk tolerance is in terms of the acceptable level of unplanned downtime. Unplanned downtime is captured by the desired level of service availability, usually expressed in terms of the number of "nines" we would like to provide: 99.9%, 99.99%, or 99.999% availability. Each additional nine corresponds to an order of magnitude improvement toward 100% availability.

For serving systems, this metric is traditionally calculated based on the proportion of system uptime:

```
                     uptime
availability = -------------------
               (uptime + downtime)
```

Using this formula over the period of a year, we can calculate the acceptable number of minutes of downtime to reach a given number of nines of availability.

At Google, however, a time-based metric for availability is usually not meaningful because we are looking across globally distributed services. Instead of using metrics around uptime, they define availability in terms of the request success rate:

```
               successful requests
availability = -------------------
                  total requests
```

Quantifying unplanned downtime as a request success rate also makes this availability metric more amenable for use in systems that do not typically serve end users directly.

### Risk Tolerance of Services

What does it mean to identify the risk rolerance of a service? In a formal environment or in the case of safety-critical systems, the risk tolerance of services is typically built directly into the basic product or service definition. At Google, services' risk tolerance tends to be less clearly defined.

To identify the risk tolerance of a service, SREs must work with the product owners to turn a set of business goals into explicit objectives to which we can engineer. In this case, the business goals we're concerned about have a direct impact on the performance and reliability of the service offered. In practice, this translation is easier said than done. While consumer services often have clear product owners, it is unusual for infrastructure services to have a similar structure of product ownership.

#### Identifying the Risk Tolerance of Consumer Services

When a product team exists, that team is usually the best resource to discuss the reliability requirements for a service. In the absence of a dedicated product team, the engineers building the system often play this role either knowingly or unknowingly.

There are many factors to consider when assessing the risk tolerance of services, such as the following:

- What level of availability is required?

- Do different types of failures have different effects on the service?

- How can we use the service cost to help locate a service on the risk continuum?

- What other service metrics are important to take into account?

**Target level of availability**

The target level of availability for a given Google service usually depends on the function it provides and how the service is positioned in the marketplace. The following list includes issues to consider:

- What level of service will the users expect?

- Does this service tie directly to revenue (either our revenue, or our customers' revenue)?

- Is this a paid service, or is it free?

- If there are competitors in the marketplace, what level of service do those competitors provide?

In this service targeted at consumers, or at enterprises?

**Types of failures**

The expected shape of failures for a given service is another important consideration. How resilient is our business to service downtime?

Which is worser for the service: A constant low rate of failures, or an occasional full-site outage? Both types of failure may result in the same absolute number of errors, but may have vastly different impacts on the business.

**Cost**

Cost is often the key factor in determining the appropriate availability target for a service. In determining the availability target for each service, we ask questions such as:

- If we were to build and operate these systems at one more nine of availability, what would our incremental increase in revenue be?

- Does this additional revenue offset the cost of reaching that level of reliability?

To make this trade-off equation more concrete, consider the following cost/benefit for an example service where ach request has equal value:

- **Proposed improvement in availability target:** 99.9% -> 99.99%

- **Proprosed increase in availability:** 0.09%

- **Service revenue:** $1M

- **Value of improved availability:** $1M * 0.0009 = $900

In this case, if the cost of improving availability by one nine is less than $900, it is worth the investment. If the cost is greater than $900, the costs will exceed the projected increase in revenue.

It may be harder to set these targets when we do not have a simple translation function between reliability and revenue. One useful strategy may be to consider the background error rate of ISPs on the Internet. If failures are being measured from the end-user perspective and it is possible to drive the error rate for the service below the background error rate, those errors will fail within the noise for a given user's Internet connection.

**Other service metrics**

Examining the risk tolerance of services in relation to metrics besides availability is often fruitful. Understanding which metrics are important and which metrics aren't important provides us with degrees of freedom when attempting to take thoughtful risks.

#### Identifying the Risk Tolerance of Infrastructure Services

The requirements for building and running infrastructure components differ from the requirements for consumer products in a number of ways. A fundamental difference is that, by definition, infrastructure components have multiple clients, often with varying needs.

**Target level of availability**

Consider Bigtable, a massive-scale distributed storage system for structured data. Some consumer services serve data directly from Bigtable in the path of a user request.

Such services need low latency and high reliability. Other teams use Bigtable as a repository for data that they use to perform offline analysis on a regular basis. These teams tend to be more concerned about throughput than reliability. Risk tolerance for these two use cases is quite distinct.

One approach to meeting the needs of boths use cases is to engineer all infrastructure services to be ultra-reliable. Given the fact that these infrastructure services also tend to aggregate huge amounts of resources, such an approach is usually far too expensive in practice. To understand the different needs of the different types of users, you can look at the desired state of the request queue for each type of Bigtable user.

**Types of failures**

The low-latency user wants Bigtable's request queues to be empty so that the system can process each oustanding request immediately upon arrival (Indeed, inefficient queuing is often a cause of high tail latency). The user concerned with offline analysis is more interested in system throughput, so that user wants request queues to never be empty. To optimise for throughput, the Bigtable system should never need to idle while waiting for its next request.

As you can see, success and failure are antithetical for these sets of users. Success for the low-latency user is failure for the user concerned with offline analysis.

**Cost**

One way to satisfy these competing constraints in a cost-effective manner is to partition the infrastructure and offer it at multiple independent levels of services.

The key strategy with regards to infrastructure is to deliver services with explicitly delineated levels of service, thus enabling the clients to make the right risk and cost trade-offs when building their systems. With explicitly delineated levels of service, the infrastructure providers can effectively externalize the difference in the cost it takes to provide service at a given level to clients.

#### Motivation for Error Budgets

Meanwhile, SRE performance is evaluated based upon reliability of a service, which implies an incentive to push back against a high rate of change. Information asymmetry between the two teams further amplifies this inherent tension. The product developers have more visibility into the time and effort involved in writing and releasing their code, while the SREs have more visibility into the service's reliability.

These tensions often reflect themselves in different opinions about the level of effort that should be put into engineering practices. The following list presents some typical tensions:

- **Software fault tolerance:** How hardened do we make the software to unexpected events? Too little, and we have a brittle, unusable product. Too much, and we have a product no one wants to use (but that runs very stably).

- **Testing:** Again, not enough testing and you have embarrasing outages, privacy data leaks, or a number of other press-worthy events. Too much testing, and you might lose your market.

- **Push frequency:** Every push is risky. How much should we work on reducing that risk, versus doing other work?

- **Canary duration and size:** It's a best practice to test a new release on some small subset of a typical workload, a practice often called canarying. How long do we wait, and how big it the canary?

**Forming Your Error Budget**

In order to base these decisions on objective data, the two teams jointly define a quarterly error budget based on the service's service level objective, or SLO. The error budget provides a clear, objetive metric that determines how unreliable the service is allowed to be within a single quarter. This metric remover thepolitics from negotiations between the SREs and the product developers when deciding how much risk to allow.

Our practice is then as follows:

- Product Management defines an SLO, which sets an expectation of how much uptime the service should have per quarter.

- The actual uptime is measured by a neutral third party: our monitoring system.

- The difference between these two numbers is the "budget" of how much "unreliability" is remaining for the quarter.

- As long as the uptime measured is above the SLO - in other words, as long as there is error budget remaining - new releases can be pushed.

**Benefits**

The main benefit of an error budget is that it provides a common incentive that allows both product development and SRE to focus on finding the right balance between innovation and reliability.

Many products use this control loop to manage release velocity: as long as the system's SLOs are met, releases can continue. If SLO violations occur frequently enough to expend the error budget, releases are temporarily halted while additional resources are invested in system testing and development to make system more resilient, improve its performance, and so on.

More subtle and effective approaches are available than this sample on/off technique: for instance, slowing down releases or rolling them back when the SLO-violation error budget is close to being used up.

**Key Insights**

- Managing service reliability is largely about managing risk, and managing risk can be costly.

- 100% is probably never the right reliability target: not only is it impossible to achieve, it's typically more reliability than a service's users want or notice. Match the profile of the service to the risk the business is willing to take.

- An error budget aligns incentives and emphasizes joint ownership between SRE and product developement. Error budgets make it easier to decide the rate of releases and to effectively defuse discussions about outages with stakeholders, and allows multiple teams to reach the same conclusion about production risk without rancor.

<!--- Current Page 78 / Last Page 78 -->